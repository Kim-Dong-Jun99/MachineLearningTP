{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Dataset imports\n",
    "import json\n",
    "\n",
    "# For restoring the dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "# Text manipulations\n",
    "import re\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cosine similarity\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Natural Language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A library for NLP.\n",
    "* Usage:\n",
    "    * nltk.corpus.**stopwords**: stopwords of specific language\n",
    "    * nltk.tokenize.**RegexpTokenizer**: Tokenize the input sentences\n",
    "    * nltk.stem.**WordNetLemmatizer**: Lemmatize the word net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redial Parser\n",
    "A separated library for parsing the redial dataset\n",
    "\n",
    "class **RedialParser**\n",
    "- Restore(): Restore train, test, and movie dataset to initial state\n",
    "   * return:\n",
    "        * None\n",
    "- Movies(train): Get movie list in dataset\n",
    "   * param:\n",
    "        * train (bool): Target dataset, (train=True, test=False, all=None)\n",
    "   * return:\n",
    "        * dict: {index, MovieName}\n",
    "- describe(): Describe its datasets\n",
    "   * return:\n",
    "        * None\n",
    "- train: Train data of ReDial.\n",
    "- test: Test data of ReDial.\n",
    "- movie: Movie mention counts for ReDial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    TODO: initialization function for dataset reads\n",
    "\n",
    "        :arg\n",
    "            path (str): Dataset path.\n",
    "        :return\n",
    "            tuple: (train, test, df_mention)\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    for line in open(f\"{path}/train_data.jsonl\", \"r\"):\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "    test_data = []\n",
    "    for line in open(f\"{path}/test_data.jsonl\", \"r\"):\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "    mention_dataframe = pd.read_csv(f\"{path}/movies_with_mentions.csv\")\n",
    "\n",
    "    return train_data, test_data, mention_dataframe\n",
    "\n",
    "\n",
    "class RedialParser:\n",
    "    def __init__(self, path):\n",
    "        self.train, self.test, self.movie = load_data(path)\n",
    "\n",
    "        self._global_movie_list = None  # list of all movie names (global movie name data)\n",
    "        self._global_msg_list = None  # list of whole lines (global line data)\n",
    "        self._local_movie_list = None  # list of movie names (local movie name data)\n",
    "        self._local_msg_list = None  # list of lines (local line data)\n",
    "\n",
    "        self.dialog_df = None  # Sum of dialogs for each movie indices\n",
    "\n",
    "        self.__train = deepcopy(self.train)\n",
    "        self.__test = deepcopy(self.test)\n",
    "        self.__movie = deepcopy(self.movie)\n",
    "\n",
    "        self.__model = None\n",
    "\n",
    "\n",
    "    def Restore(self):\n",
    "        \"\"\"\n",
    "        TODO: Restore train, test, and movie dataset to initial state\n",
    "        \"\"\"\n",
    "        self.train = deepcopy(self.__train)\n",
    "        self.test = deepcopy(self.__test)\n",
    "        self.movie = deepcopy(self.__movie)\n",
    "\n",
    "\n",
    "    def Movies(self, train=True) -> dict:\n",
    "        \"\"\"\n",
    "        TODO: Get movie list in dataset\n",
    "\n",
    "            :arg\n",
    "                train (bool): Target dataset, (train=True, test=False, all=None)\n",
    "            :return\n",
    "                dict: {index, MovieName}\n",
    "        \"\"\"\n",
    "        if train is None:\n",
    "            result = self.Movies()\n",
    "            result.update(self.Movies(False))\n",
    "            return result\n",
    "\n",
    "        target = None\n",
    "        if train is True:\n",
    "            target = self.train\n",
    "        elif train is False:\n",
    "            target = self.test\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if target is not None:\n",
    "            for elem in target:\n",
    "                result.update(elem['movieMentions'])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        TODO: Describe its datasets\n",
    "        \"\"\"\n",
    "        len1, len2 = len(self.train), len(self.test)\n",
    "        n1, n2 = 0, 0\n",
    "        m1, m2 = 0, 0\n",
    "\n",
    "        for e in self.train:\n",
    "            n1 += len(e['movieMentions'])\n",
    "            m1 += len(e['messages'])\n",
    "        for e in self.test:\n",
    "            n2 += len(e['movieMentions'])\n",
    "            m2 += len(e['messages'])\n",
    "\n",
    "        print('Brief information:\\n'\n",
    "              f'Length of train data: {len1}\\n'\n",
    "              f'Length of test data: {len2}\\n\\n'\n",
    "              'Data information:\\n'\n",
    "              f'Key parameters: {list(self.train[0].keys())}\\n'\n",
    "              f'Key parameters in Questions: {list(list(self.train[0][\"respondentQuestions\"].values())[0].keys())}\\n'\n",
    "              f'Key parameters in messages: {list(self.train[0][\"messages\"][0].keys())}\\n\\n'\n",
    "              'Context information:\\n'\n",
    "              f'Total mentioned movie number (train): {n1}\\n'\n",
    "              f'Total mentioned movie number in unique (train): {len(self.Movies())}\\n'\n",
    "              f'Total message number (train): {m1}\\n'\n",
    "              f'Total mentioned movie number (test): {n2}\\n'\n",
    "              f'Total mentioned movie number in unique (test): {len(self.Movies(False))}\\n'\n",
    "              f'Total message number (test): {m2}\\n'\n",
    "              f'Average mentioned movie numbers per conversation (train): {n1 / len1}\\n'\n",
    "              f'Average message numbers per conversation (train): {m1 / len1}\\n'\n",
    "              f'Average mentioned movie numbers per conversation (test): {n2 / len2}\\n'\n",
    "              f'Average message numbers per conversation (test): {m2 / len2}\\n\\n'\n",
    "              , end='')\n",
    "    \n",
    "\n",
    "    def preprocessing(self):\n",
    "        \"\"\"\n",
    "        TODO: Regroup train dataset into purposed structure and clean up data\n",
    "        \"\"\"\n",
    "        compile = re.compile(\"\\W+\")  # Format\n",
    "        \n",
    "        ran = range(len(self.train))\n",
    "\n",
    "        # initialize list\n",
    "        self._global_movie_list = []\n",
    "        self._global_msg_list = []\n",
    "        self._local_movie_list = [[] for _ in ran]\n",
    "        self._local_msg_list = [[] for _ in ran]\n",
    "\n",
    "        for i, data in enumerate(self.train):\n",
    "            for msg in data['messages']:  # append line to the lists\n",
    "                self._local_msg_list[i].append(msg['text'])\n",
    "                self._global_msg_list.append(msg['text'])\n",
    "\n",
    "            # Extract movie indices\n",
    "            for idx, line in enumerate(self._local_msg_list[i]):\n",
    "                numbers = re.findall(r'@\\d+', line)  # find number keywords (ex: @12345)\n",
    "                for number in numbers:\n",
    "                    self._local_movie_list[i].append(number[1:])\n",
    "                    self._global_movie_list.append(number[1:])\n",
    "\n",
    "                    # Remove index string\n",
    "                    pos = line.index(number)\n",
    "                    line = self._local_msg_list[i][idx] = line[0: pos] + line[pos + len(number): len(line)]\n",
    "\n",
    "                # Post: clear meaningless words\n",
    "                a = compile.sub(\" \", line)  # Clear special character\n",
    "                self._local_msg_list[i][idx] = a.lower()  # lower character\n",
    "\n",
    "        # Construct dialog dataframe\n",
    "        self.dialog_df = pd.DataFrame(columns=[\"movieid\", \"dialog\"])\n",
    "\n",
    "        for lines, movies in zip(self._local_msg_list, self._local_movie_list):\n",
    "            dig = ''\n",
    "            for line in lines:  # concatenate all sentences in related message dialog\n",
    "                dig += ' ' + str(line)\n",
    "            \n",
    "            for mv in movies:\n",
    "                newrow = pd.DataFrame({'movieid': [mv], 'dialog': [dig]}, columns=self.dialog_df.columns)\n",
    "                self.dialog_df = pd.concat([self.dialog_df, newrow], ignore_index=True)\n",
    "        \n",
    "        # Fill NaN with empty sentence\n",
    "        self.dialog_df['dialog'].fillna('', inplace=True)\n",
    "    \n",
    "\n",
    "    def get_frequency_matrix(self, tags):\n",
    "        \"\"\"\n",
    "        TODO: compute the frequency of tag words to obtain the TF-IDFs matrix\n",
    "\n",
    "            :arg\n",
    "                tags (list): list of key words.\n",
    "            :return\n",
    "                pandas.DataFrame: frequency matrix of tag words.\n",
    "        \"\"\"\n",
    "        stop_word_eng = set(stopwords.words('english'))\n",
    "        ran = range(len(self.train))\n",
    "\n",
    "        msg_list = deepcopy(self._local_msg_list)\n",
    "\n",
    "        for i in ran:\n",
    "            msg_list[i] = [j for j in msg_list[i] if j not in stop_word_eng]  # Clear stopwords\n",
    "\n",
    "        # Lemmatizer class\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "        # mv_tags = ['comedy','scary','love','animation','artistic','war','sci','blood','hero','romantic','action']\n",
    "        x = pd.DataFrame(columns=['id'] + tags)\n",
    "\n",
    "        for idx, msg in enumerate(msg_list):\n",
    "            result_pre_lem = [token.tokenize(j) for j in msg]\n",
    "            middle_pre_lem = [r for j in result_pre_lem for r in j]\n",
    "            final_lem = [lemmatizer.lemmatize(j) for j in middle_pre_lem if not j in stop_word_eng]  # Remove stopword\n",
    "\n",
    "            # Lemmatization\n",
    "            english = pd.Series(final_lem)\n",
    "            for word in english:\n",
    "                if word in tags:\n",
    "                    for movie in self._local_movie_list[idx]:\n",
    "                        if x[x['id'] == movie].empty:\n",
    "                            new_row = pd.DataFrame({'id': [movie]}, columns=x.columns)\n",
    "                            x = pd.concat([x, new_row], ignore_index=True)\n",
    "                            x.fillna(0, inplace=True)\n",
    "                        x.loc[x['id'] == movie, word] += 1\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_tfidf_matrix(self, **tfidf_keys):\n",
    "        \"\"\"\n",
    "        TODO: Compute TF-IDFs matrix\n",
    "\n",
    "            :arg\n",
    "                tfidf_keys(keyword dict): TfidfVectorizer parameters\n",
    "            :return\n",
    "                numpy.ndrarry: TF-IDFs matrix\n",
    "                numpy.ndarray: feature name of TF-IDFs (word)\n",
    "        \"\"\"\n",
    "        # Vectorizer class\n",
    "        tfidf = TfidfVectorizer(**tfidf_keys)  # Ignore English Stopwords\n",
    "\n",
    "        # Obtain matrix\n",
    "        tfidf_df = tfidf.fit_transform(self.dialog_df['dialog'])\n",
    "\n",
    "        return tfidf_df.toarray(), tfidf.get_feature_names_out()\n",
    "\n",
    "\n",
    "    def __max_sum_sim(self, doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "        \"\"\"\n",
    "        TODO: Minimize redundancy and maximize diversity - Get {top_n} keyword(s) using Cosine Similarity\n",
    "\n",
    "            :arg\n",
    "                doc_embedding(numpy.ndarray): Document embedding (Obtained from dialog)\n",
    "                candidate_embeddings(numpy.ndarray): Candidates embedding (Obtained from CountVectorizer)\n",
    "                words(numpy.ndarray): Candidates\n",
    "                top_n(int): Maximum size of high similarity indices\n",
    "                nr_candidates(int): Nearby Candidates, the word list has maximum size is this.\n",
    "            :return\n",
    "                list: list of high similarity keywords with length = {top_n}\n",
    "        \"\"\"\n",
    "        # Compute similarity of document and candidates\n",
    "        distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "        # Compute similarity of candidates\n",
    "        distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "        # Explict {top_n} words\n",
    "        words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "        words_vals = [words[index] for index in words_idx]\n",
    "        distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "        # Compute the combination of keywords having minimum redundancy\n",
    "        min_sim = np.inf\n",
    "        candidate = None\n",
    "        for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "            sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "            if sim < min_sim:\n",
    "                candidate = combination\n",
    "                min_sim = sim\n",
    "\n",
    "        return [words_vals[idx] for idx in candidate]\n",
    "    \n",
    "    \n",
    "    def __mmr(self, doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "        \"\"\"\n",
    "        TODO: Minimize redundancy and maximize diversity - Get {top_n} keyword(s) using MMR\n",
    "\n",
    "            :arg\n",
    "                doc_embedding(numpy.ndarray): Document embedding (Obtained from dialog)\n",
    "                candidate_embeddings(numpy.ndarray): Candidates embedding (Obtained from CountVectorizer)\n",
    "                words(numpy.ndarray): Candidates\n",
    "                top_n(int): Maximum size of high similarity indices\n",
    "                diversity(float): Diversity of candidates, higher diversity has larger candidates num.\n",
    "            :return\n",
    "                list: list of high similarity keywords with length = {top_n}\n",
    "        \"\"\"\n",
    "        # Word - document similarity\n",
    "        word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "        # Candidates - similarity\n",
    "        word_similarity = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "        # Extract the simliarest word's index\n",
    "        keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "        # Index list excluding the similarest index\n",
    "        candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "        # Repeat this for remaining counts (top_n - 1)\n",
    "        for _ in range(top_n - 1):\n",
    "            candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "            print(word_similarity.shape, candidates_idx, keywords_idx)\n",
    "            target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "            # Compute the MMR\n",
    "            mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "            mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "            # Update indices\n",
    "            keywords_idx.append(mmr_idx)\n",
    "            candidates_idx.remove(mmr_idx)\n",
    "\n",
    "        return [words[idx] for idx in keywords_idx]\n",
    "    \n",
    "\n",
    "    def BERT(self, bert_model, top_n):\n",
    "        \"\"\"\n",
    "            TODO: Do *BERT* model using CountVectorizer\n",
    "\n",
    "            :arg\n",
    "                bert_model(str): Sentence Transofmer name, see more information in https://huggingface.co/models?library=sentence-transformers\n",
    "                top_n(int): The number of high similarity words from candidates\n",
    "            :return\n",
    "                pandas.DataFrame: Movie index and {top_n} words with high similarity using cosine_similarity and MMR.\n",
    "        \"\"\"\n",
    "        num = len(self.dialog_df['movieid'])\n",
    "\n",
    "        n_gram_range = (3, 3)\n",
    "        \n",
    "        df = pd.DataFrame(columns=['movieid', 'Words(cosine_similarity)', 'Words(MMR)'])\n",
    "\n",
    "        for i in range(num):\n",
    "            count = CountVectorizer(ngram_range=n_gram_range, stop_words='english').fit(self.dialog_df.iloc[i, :].values)\n",
    "            candidates = count.get_feature_names_out()\n",
    "\n",
    "            model = SentenceTransformer(bert_model)\n",
    "\n",
    "            doc_embedding = model.encode(self.dialog_df.iloc[i, :].values)\n",
    "            candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "            (csim, mmr) = (self.__max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, 20),\n",
    "                self.__mmr(doc_embedding, candidate_embeddings, candidates, top_n, 0.7))\n",
    "\n",
    "            newrow = pd.DataFrame({\n",
    "                'movieid': self.dialog_df.iloc[i, 0].values,\n",
    "                'Words(cosine_similarity)': [csim],\n",
    "                'Words(MMR)': [mmr]},\n",
    "                columns=df.columns)\n",
    "            df = pd.concat([df, newrow], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def similarity(X, Y):\n",
    "        \"\"\"\n",
    "        TODO: Compute the cosine simliarity between X and Y. For avoiding the DivByZero, the denominator has 1e-7 minimum value.\n",
    "\n",
    "            :arg\n",
    "                X (numpy.ndarray): X data array\n",
    "                Y (numpy.ndarray): Y data array\n",
    "            :return\n",
    "                float\n",
    "        \"\"\"\n",
    "        return np.dot(X, Y) / ((norm(X) * norm(Y)) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "Import dataset, describe it briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brief information:\n",
      "Length of train data: 10006\n",
      "Length of test data: 1342\n",
      "\n",
      "Data information:\n",
      "Key parameters: ['movieMentions', 'respondentQuestions', 'messages', 'conversationId', 'respondentWorkerId', 'initiatorWorkerId', 'initiatorQuestions']\n",
      "Key parameters in Questions: ['suggested', 'seen', 'liked']\n",
      "Key parameters in messages: ['timeOffset', 'text', 'senderWorkerId', 'messageId']\n",
      "\n",
      "Context information:\n",
      "Total mentioned movie number (train): 52918\n",
      "Total mentioned movie number in unique (train): 6223\n",
      "Total message number (train): 182150\n",
      "Total mentioned movie number (test): 7154\n",
      "Total mentioned movie number in unique (test): 2007\n",
      "Total message number (test): 23952\n",
      "Average mentioned movie numbers per conversation (train): 5.288626823905656\n",
      "Average message numbers per conversation (train): 18.20407755346792\n",
      "Average mentioned movie numbers per conversation (test): 5.330849478390462\n",
      "Average message numbers per conversation (test): 17.847988077496275\n",
      "\n",
      "length of train dataset: 10006\n"
     ]
    }
   ],
   "source": [
    "parser = RedialParser('../dataset')\n",
    "parser.describe()  # Describe read dataset\n",
    "\n",
    "# Size of train data\n",
    "num = len(parser.train)\n",
    "print(f'length of train dataset: {num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Clear the special character and extract the text and movie indices\n",
    "- example: \"I like animations like @84779 and @191602\" → [i like animations like  and ], [84779, 191602]\n",
    "\n",
    "\n",
    "Specific:\n",
    "* Transform dataset structure.\n",
    "    * Original: [movieMentions, {messages}, conversationId, ...]\n",
    "    * Transformed: [movie_indices], [message_contexts], [[1st_movie_index], [2nd_...], ...], [[1st_message_context], [2nd_...], ...]\n",
    "    * Dialog Dataframe (*self.dialog_df*): {'movie_id': '1st message' + '2nd message' + ...} - Used in generation of **TF-IDF** matrix\n",
    "* Recognize movie indices\n",
    "    * **@** recognition: use re library's *findall(@\\d+)* function, it only detects '@' + index strings.\n",
    "* Clean up meaningless values\n",
    "    * Special characters: use re library's format *\\w+*, it only receives widechar characters.\n",
    "    * Movie index: remove context of them by using text slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieid</th>\n",
       "      <th>dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84779</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191602</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122159</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165710</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151313</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64456</th>\n",
       "      <td>204974</td>\n",
       "      <td>what type of movies do you like  hi i m looki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64457</th>\n",
       "      <td>85036</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64458</th>\n",
       "      <td>170277</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64459</th>\n",
       "      <td>149938</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64460</th>\n",
       "      <td>200018</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieid                                             dialog\n",
       "0       84779   hi there how are you i m looking for movie re...\n",
       "1      191602   hi there how are you i m looking for movie re...\n",
       "2      122159   hi there how are you i m looking for movie re...\n",
       "3      165710   hi there how are you i m looking for movie re...\n",
       "4      151313   hi there how are you i m looking for movie re...\n",
       "...       ...                                                ...\n",
       "64456  204974   what type of movies do you like  hi i m looki...\n",
       "64457   85036   hello  hi how can i help you so some of the m...\n",
       "64458  170277   hello  hi how can i help you so some of the m...\n",
       "64459  149938   hello  hi how can i help you so some of the m...\n",
       "64460  200018   hello  hi how can i help you so some of the m...\n",
       "\n",
       "[64461 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.preprocessing()\n",
    "parser.dialog_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "* 1. Extract words and their counts related to the movies. (Did not used, only for eye inspection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comedy</th>\n",
       "      <th>scary</th>\n",
       "      <th>love</th>\n",
       "      <th>animation</th>\n",
       "      <th>artistic</th>\n",
       "      <th>war</th>\n",
       "      <th>sci</th>\n",
       "      <th>blood</th>\n",
       "      <th>hero</th>\n",
       "      <th>romantic</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84779</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191602</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122159</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165710</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151313</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>205981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>106113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>96852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>112404</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>200018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5101 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  comedy  scary  love  animation  artistic  war  sci  blood  hero  \\\n",
       "0      84779       2      0     0          1         1    0    1      0     0   \n",
       "1     191602       2      0     0          1         1    0    1      0     0   \n",
       "2     122159       2      0     0          1         1    0    1      0     0   \n",
       "3     165710      37      1    95         13         1    0    4      0     1   \n",
       "4     151313       3      0     2          3         1    0    1      0     0   \n",
       "...      ...     ...    ...   ...        ...       ...  ...  ...    ...   ...   \n",
       "5096  205981       0      0     0          1         0    0    0      0     0   \n",
       "5097  106113       0      0     0          0         0    0    3      0     0   \n",
       "5098   96852       0      0     0          0         0    0    0      0     0   \n",
       "5099  112404       0      0     0          0         0    0    0      0     0   \n",
       "5100  200018       0      0     0          0         0    0    0      0     0   \n",
       "\n",
       "      romantic  action  \n",
       "0            0       1  \n",
       "1            0       1  \n",
       "2            0       1  \n",
       "3            0      13  \n",
       "4            0       2  \n",
       "...        ...     ...  \n",
       "5096         0       0  \n",
       "5097         0       1  \n",
       "5098         0       1  \n",
       "5099         0       1  \n",
       "5100         0       4  \n",
       "\n",
       "[5101 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag words words related with movie genres\n",
    "mv_tags = ['comedy','scary','love','animation','artistic','war','sci','blood','hero','romantic','action']\n",
    "frequency = parser.get_frequency_matrix(mv_tags)\n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. Normal TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bye</th>\n",
       "      <th>check</th>\n",
       "      <th>comedy</th>\n",
       "      <th>day</th>\n",
       "      <th>did</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>funny</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>...</th>\n",
       "      <th>saw</th>\n",
       "      <th>seen</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>sure</th>\n",
       "      <th>thank</th>\n",
       "      <th>thanks</th>\n",
       "      <th>think</th>\n",
       "      <th>ve</th>\n",
       "      <th>watch</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84779</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191602</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122159</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165710</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151313</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64456</th>\n",
       "      <td>204974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182008</td>\n",
       "      <td>0.085112</td>\n",
       "      <td>0.297298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64457</th>\n",
       "      <td>85036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253538</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.178612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64458</th>\n",
       "      <td>170277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253538</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.178612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64459</th>\n",
       "      <td>149938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253538</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.178612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64460</th>\n",
       "      <td>200018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253538</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.178612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64461 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       bye check    comedy     day       did     enjoy     funny  \\\n",
       "0       84779  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "1      191602  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "2      122159  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "3      165710  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "4      151313  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "...       ...       ...   ...       ...     ...       ...       ...       ...   \n",
       "64456  204974       0.0   0.0       0.0     0.0  0.173777       0.0       0.0   \n",
       "64457   85036       0.0   0.0       0.0  0.4115       0.0  0.248506       0.0   \n",
       "64458  170277       0.0   0.0       0.0  0.4115       0.0  0.248506       0.0   \n",
       "64459  149938       0.0   0.0       0.0  0.4115       0.0  0.248506       0.0   \n",
       "64460  200018       0.0   0.0       0.0  0.4115       0.0  0.248506       0.0   \n",
       "\n",
       "           good     great  ...       saw      seen suggestions      sure  \\\n",
       "0       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "1       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "2       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "3       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "4       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "...         ...       ...  ...       ...       ...         ...       ...   \n",
       "64456  0.242174       0.0  ...  0.182008  0.085112    0.297298       0.0   \n",
       "64457  0.170075  0.070963  ...  0.127822  0.059773         0.0  0.253538   \n",
       "64458  0.170075  0.070963  ...  0.127822  0.059773         0.0  0.253538   \n",
       "64459  0.170075  0.070963  ...  0.127822  0.059773         0.0  0.253538   \n",
       "64460  0.170075  0.070963  ...  0.127822  0.059773         0.0  0.253538   \n",
       "\n",
       "          thank    thanks     think   ve     watch       yes  \n",
       "0           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "1           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "2           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "3           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "4           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "...         ...       ...       ...  ...       ...       ...  \n",
       "64456       0.0   0.11514       0.0  0.0  0.139028       0.0  \n",
       "64457  0.290231       0.0  0.304353  0.0  0.292913  0.178612  \n",
       "64458  0.290231       0.0  0.304353  0.0  0.292913  0.178612  \n",
       "64459  0.290231       0.0  0.304353  0.0  0.292913  0.178612  \n",
       "64460  0.290231       0.0  0.304353  0.0  0.292913  0.178612  \n",
       "\n",
       "[64461 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat, tfidf_columns = parser.get_tfidf_matrix(stop_words='english', min_df=0.2)\n",
    "\n",
    "# Construct dataset with id + word vectors\n",
    "cdata = np.concatenate((parser.dialog_df['movieid'].to_numpy().reshape(len(parser.dialog_df['dialog']), 1), tfidf_mat), axis=1)\n",
    "df_mv_tfidf = pd.DataFrame(cdata, columns=['id'] + tfidf_columns.tolist())\n",
    "df_mv_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 68 is out of bounds for axis 1 with size 57",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bert_df \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mBERT(\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-nli-mean-tokens\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m bert_df\n",
      "Cell \u001b[1;32mIn [4], line 320\u001b[0m, in \u001b[0;36mRedialParser.BERT\u001b[1;34m(self, bert_model, top_n)\u001b[0m\n\u001b[0;32m    316\u001b[0m doc_embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialog_df\u001b[39m.\u001b[39miloc[i, :]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m    317\u001b[0m candidate_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(candidates)\n\u001b[0;32m    319\u001b[0m (csim, mmr) \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, \u001b[39m20\u001b[39m),\n\u001b[1;32m--> 320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__mmr(doc_embedding, candidate_embeddings, candidates, top_n, \u001b[39m0.7\u001b[39;49m))\n\u001b[0;32m    322\u001b[0m newrow \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[0;32m    323\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmovieid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialog_df\u001b[39m.\u001b[39miloc[i, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalues,\n\u001b[0;32m    324\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mWords(cosine_similarity)\u001b[39m\u001b[39m'\u001b[39m: [csim],\n\u001b[0;32m    325\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mWords(MMR)\u001b[39m\u001b[39m'\u001b[39m: [mmr]},\n\u001b[0;32m    326\u001b[0m     columns\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    327\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df, newrow], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [4], line 281\u001b[0m, in \u001b[0;36mRedialParser.__mmr\u001b[1;34m(self, doc_embedding, candidate_embeddings, words, top_n, diversity)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(top_n \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    280\u001b[0m     candidate_similarities \u001b[39m=\u001b[39m word_doc_similarity[candidates_idx, :]\n\u001b[1;32m--> 281\u001b[0m     target_similarities \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(word_similarity[candidates_idx][:, keywords_idx], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    283\u001b[0m     \u001b[39m# Compute the MMR\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     mmr \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m diversity) \u001b[39m*\u001b[39m candidate_similarities \u001b[39m-\u001b[39m diversity \u001b[39m*\u001b[39m target_similarities\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 68 is out of bounds for axis 1 with size 57"
     ]
    }
   ],
   "source": [
    "bert_df = parser.BERT('distilbert-base-nli-mean-tokens', 5)\n",
    "bert_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Metrics\n",
    "* Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation function\n",
    "* param:\n",
    "    * data: array, vector space of texts.\n",
    "    * mv: target movie's index\n",
    "    * length: maximum length of recommendation\n",
    "        * default: 5\n",
    "    * simf: consine similarity function\n",
    "        * default: dot(X, y) / (normalize(X) * normalize(Y) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the consine similarity function's denominator has 1e-7 minimum value to avoid the divbyzero.\n",
    "def recommend(matrix, index, length=5, simf):\n",
    "    sim = []\n",
    "\n",
    "    if df.loc[df['movieid'] == mv].empty:\n",
    "        return sim\n",
    "    \n",
    "    idx = df[df['movieid'] == mv].index.values[0]\n",
    "\n",
    "    for idx, data in enumerate(matrix):\n",
    "        if idx != i:\n",
    "            sim.append((simf(data[i], data[idx]), df.loc[i]['movieid']))\n",
    "    \n",
    "    sim.sort()\n",
    "    sim.reverse()\n",
    "    return sim[:length]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
