{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Dataset imports\n",
    "import json\n",
    "\n",
    "# For restoring the dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "# Text manipulations\n",
    "import re\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cosine similarity\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Natural Language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A library for NLP.\n",
    "* Usage:\n",
    "    * nltk.corpus.**stopwords**: stopwords of specific language\n",
    "    * nltk.tokenize.**RegexpTokenizer**: Tokenize the input sentences\n",
    "    * nltk.stem.**WordNetLemmatizer**: Lemmatize the word net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redial Parser\n",
    "A separated library for parsing the redial dataset\n",
    "\n",
    "class **RedialParser**\n",
    "- Restore(): Restore train, test, and movie dataset to initial state\n",
    "   * return:\n",
    "        * None\n",
    "- Movies(train): Get movie list in dataset\n",
    "   * param:\n",
    "        * train (bool): Target dataset, (train=True, test=False, all=None)\n",
    "   * return:\n",
    "        * dict: {index, MovieName}\n",
    "- describe(): Describe its datasets\n",
    "   * return:\n",
    "        * None\n",
    "- train: Train data of ReDial.\n",
    "- test: Test data of ReDial.\n",
    "- movie: Movie mention counts for ReDial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    TODO: initialization function for dataset reads\n",
    "\n",
    "        :arg\n",
    "            path (str): Dataset path.\n",
    "        :return\n",
    "            tuple: (train, test, df_mention)\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    for line in open(f\"{path}/train_data.jsonl\", \"r\"):\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "    test_data = []\n",
    "    for line in open(f\"{path}/test_data.jsonl\", \"r\"):\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "    mention_dataframe = pd.read_csv(f\"{path}/movies_with_mentions.csv\")\n",
    "\n",
    "    return train_data, test_data, mention_dataframe\n",
    "\n",
    "\n",
    "class RedialParser:\n",
    "    def __init__(self, path):\n",
    "        self.train, self.test, self.movie = load_data(path)\n",
    "\n",
    "        self._global_movie_list = None  # list of all movie names (global movie name data)\n",
    "        self._global_msg_list = None  # list of whole lines (global line data)\n",
    "        self._local_movie_list = None  # list of movie names (local movie name data)\n",
    "        self._local_msg_list = None  # list of lines (local line data)\n",
    "\n",
    "        self.dialog_df = None  # Sum of dialogs for each movie indices\n",
    "\n",
    "        self.__train = deepcopy(self.train)\n",
    "        self.__test = deepcopy(self.test)\n",
    "        self.__movie = deepcopy(self.movie)\n",
    "\n",
    "        self.__model = None\n",
    "\n",
    "\n",
    "    def Restore(self):\n",
    "        \"\"\"\n",
    "        TODO: Restore train, test, and movie dataset to initial state\n",
    "        \"\"\"\n",
    "        self.train = deepcopy(self.__train)\n",
    "        self.test = deepcopy(self.__test)\n",
    "        self.movie = deepcopy(self.__movie)\n",
    "\n",
    "\n",
    "    def Movies(self, train=True) -> dict:\n",
    "        \"\"\"\n",
    "        TODO: Get movie list in dataset\n",
    "\n",
    "            :arg\n",
    "                train (bool): Target dataset, (train=True, test=False, all=None)\n",
    "            :return\n",
    "                dict: {index, MovieName}\n",
    "        \"\"\"\n",
    "        if train is None:\n",
    "            result = self.Movies()\n",
    "            result.update(self.Movies(False))\n",
    "            return result\n",
    "\n",
    "        target = None\n",
    "        if train is True:\n",
    "            target = self.train\n",
    "        elif train is False:\n",
    "            target = self.test\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if target is not None:\n",
    "            for elem in target:\n",
    "                result.update(elem['movieMentions'])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        TODO: Describe its datasets\n",
    "        \"\"\"\n",
    "        len1, len2 = len(self.train), len(self.test)\n",
    "        n1, n2 = 0, 0\n",
    "        m1, m2 = 0, 0\n",
    "\n",
    "        for e in self.train:\n",
    "            n1 += len(e['movieMentions'])\n",
    "            m1 += len(e['messages'])\n",
    "        for e in self.test:\n",
    "            n2 += len(e['movieMentions'])\n",
    "            m2 += len(e['messages'])\n",
    "\n",
    "        print('Brief information:\\n'\n",
    "              f'Length of train data: {len1}\\n'\n",
    "              f'Length of test data: {len2}\\n\\n'\n",
    "              'Data information:\\n'\n",
    "              f'Key parameters: {list(self.train[0].keys())}\\n'\n",
    "              f'Key parameters in Questions: {list(list(self.train[0][\"respondentQuestions\"].values())[0].keys())}\\n'\n",
    "              f'Key parameters in messages: {list(self.train[0][\"messages\"][0].keys())}\\n\\n'\n",
    "              'Context information:\\n'\n",
    "              f'Total mentioned movie number (train): {n1}\\n'\n",
    "              f'Total mentioned movie number in unique (train): {len(self.Movies())}\\n'\n",
    "              f'Total message number (train): {m1}\\n'\n",
    "              f'Total mentioned movie number (test): {n2}\\n'\n",
    "              f'Total mentioned movie number in unique (test): {len(self.Movies(False))}\\n'\n",
    "              f'Total message number (test): {m2}\\n'\n",
    "              f'Average mentioned movie numbers per conversation (train): {n1 / len1}\\n'\n",
    "              f'Average message numbers per conversation (train): {m1 / len1}\\n'\n",
    "              f'Average mentioned movie numbers per conversation (test): {n2 / len2}\\n'\n",
    "              f'Average message numbers per conversation (test): {m2 / len2}\\n\\n'\n",
    "              , end='')\n",
    "    \n",
    "\n",
    "    def preprocessing(self):\n",
    "        \"\"\"\n",
    "        TODO: Regroup train dataset into purposed structure and clean up data\n",
    "        \"\"\"\n",
    "        compile = re.compile(\"\\W+\")  # Format\n",
    "        \n",
    "        ran = range(len(self.train))\n",
    "\n",
    "        # initialize list\n",
    "        self._global_movie_list = []\n",
    "        self._global_msg_list = []\n",
    "        self._local_movie_list = [[] for _ in ran]\n",
    "        self._local_msg_list = [[] for _ in ran]\n",
    "\n",
    "        for i, data in enumerate(self.train):\n",
    "            for msg in data['messages']:  # append line to the lists\n",
    "                self._local_msg_list[i].append(msg['text'])\n",
    "                self._global_msg_list.append(msg['text'])\n",
    "\n",
    "            # Extract movie indices\n",
    "            for idx, line in enumerate(self._local_msg_list[i]):\n",
    "                numbers = re.findall(r'@\\d+', line)  # find number keywords (ex: @12345)\n",
    "                for number in numbers:\n",
    "                    self._local_movie_list[i].append(number[1:])\n",
    "                    self._global_movie_list.append(number[1:])\n",
    "\n",
    "                    # Remove index string\n",
    "                    pos = line.index(number)\n",
    "                    line = self._local_msg_list[i][idx] = line[0: pos] + line[pos + len(number): len(line)]\n",
    "\n",
    "                # Post: clear meaningless words\n",
    "                a = compile.sub(\" \", line)  # Clear special character\n",
    "                self._local_msg_list[i][idx] = a.lower()  # lower character\n",
    "\n",
    "        # Construct dialog dataframe\n",
    "        self.dialog_df = pd.DataFrame(columns=[\"movieid\", \"dialog\"])\n",
    "\n",
    "        for lines, movies in zip(self._local_msg_list, self._local_movie_list):\n",
    "            dig = ''\n",
    "            for line in lines:  # concatenate all sentences in related message dialog\n",
    "                dig += ' ' + str(line)\n",
    "            \n",
    "            for mv in movies:\n",
    "                newrow = pd.DataFrame({'movieid': [mv], 'dialog': [dig]}, columns=self.dialog_df.columns)\n",
    "                self.dialog_df = pd.concat([self.dialog_df, newrow], ignore_index=True)\n",
    "        \n",
    "        # Fill NaN with empty sentence\n",
    "        self.dialog_df['dialog'].fillna('', inplace=True)\n",
    "    \n",
    "\n",
    "    def get_frequency_matrix(self, tags):\n",
    "        \"\"\"\n",
    "        TODO: compute the frequency of tag words to obtain the TF-IDFs matrix\n",
    "\n",
    "            :arg\n",
    "                tags (list): list of key words.\n",
    "            :return\n",
    "                pandas.DataFrame: frequency matrix of tag words.\n",
    "        \"\"\"\n",
    "        stop_word_eng = set(stopwords.words('english'))\n",
    "        ran = range(len(self.train))\n",
    "\n",
    "        msg_list = deepcopy(self._local_msg_list)\n",
    "\n",
    "        for i in ran:\n",
    "            msg_list[i] = [j for j in msg_list[i] if j not in stop_word_eng]  # Clear stopwords\n",
    "\n",
    "        # Lemmatizer class\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "        # mv_tags = ['comedy','scary','love','animation','artistic','war','sci','blood','hero','romantic','action']\n",
    "        x = pd.DataFrame(columns=['id'] + tags)\n",
    "\n",
    "        for idx, msg in enumerate(msg_list):\n",
    "            result_pre_lem = [token.tokenize(j) for j in msg]\n",
    "            middle_pre_lem = [r for j in result_pre_lem for r in j]\n",
    "            final_lem = [lemmatizer.lemmatize(j) for j in middle_pre_lem if not j in stop_word_eng]  # Remove stopword\n",
    "\n",
    "            # Lemmatization\n",
    "            english = pd.Series(final_lem)\n",
    "            for word in english:\n",
    "                if word in tags:\n",
    "                    for movie in self._local_movie_list[idx]:\n",
    "                        if x[x['id'] == movie].empty:\n",
    "                            new_row = pd.DataFrame({'id': [movie]}, columns=x.columns)\n",
    "                            x = pd.concat([x, new_row], ignore_index=True)\n",
    "                            x.fillna(0, inplace=True)\n",
    "                        x.loc[x['id'] == movie, word] += 1\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_tfidf_matrix(self, **tfidf_keys):\n",
    "        \"\"\"\n",
    "        TODO: Compute TF-IDFs matrix\n",
    "\n",
    "            :arg\n",
    "                tfidf_keys(keyword dict): TfidfVectorizer parameters\n",
    "            :return\n",
    "                numpy.ndrarry: TF-IDFs matrix\n",
    "                numpy.ndarray: feature name of TF-IDFs (word)\n",
    "        \"\"\"\n",
    "        # Vectorizer class\n",
    "        tfidf = TfidfVectorizer(**tfidf_keys)  # Ignore English Stopwords\n",
    "\n",
    "        # Obtain matrix\n",
    "        tfidf_df = tfidf.fit_transform(self.dialog_df['dialog'])\n",
    "\n",
    "        return tfidf_df.toarray(), tfidf.get_feature_names_out()\n",
    "\n",
    "\n",
    "    def __max_sum_sim(self, doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "        \"\"\"\n",
    "        TODO: Minimize redundancy and maximize diversity - Get {top_n} keyword(s) using Cosine Similarity\n",
    "\n",
    "            :arg\n",
    "                doc_embedding(numpy.ndarray): Document embedding (Obtained from dialog)\n",
    "                candidate_embeddings(numpy.ndarray): Candidates embedding (Obtained from CountVectorizer)\n",
    "                words(numpy.ndarray): Candidates\n",
    "                top_n(int): Maximum size of high similarity indices\n",
    "                nr_candidates(int): Nearby Candidates, the word list has maximum size is this.\n",
    "            :return\n",
    "                list: list of high similarity keywords with length = {top_n}\n",
    "        \"\"\"\n",
    "        # Compute similarity of document and candidates\n",
    "        distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "        # Compute similarity of candidates\n",
    "        distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "        # Explict {top_n} words\n",
    "        words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "        words_vals = [words[index] for index in words_idx]\n",
    "        distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "        # Compute the combination of keywords having minimum redundancy\n",
    "        min_sim = np.inf\n",
    "        candidate = None\n",
    "        for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "            sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "            if sim < min_sim:\n",
    "                candidate = combination\n",
    "                min_sim = sim\n",
    "\n",
    "        return [words_vals[idx] for idx in candidate]\n",
    "    \n",
    "    \n",
    "    def __mmr(self, doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "        \"\"\"\n",
    "        TODO: Minimize redundancy and maximize diversity - Get {top_n} keyword(s) using MMR\n",
    "\n",
    "            :arg\n",
    "                doc_embedding(numpy.ndarray): Document embedding (Obtained from dialog)\n",
    "                candidate_embeddings(numpy.ndarray): Candidates embedding (Obtained from CountVectorizer)\n",
    "                words(numpy.ndarray): Candidates\n",
    "                top_n(int): Maximum size of high similarity indices\n",
    "                diversity(float): Diversity of candidates, higher diversity has larger candidates num.\n",
    "            :return\n",
    "                list: list of high similarity keywords with length = {top_n}\n",
    "        \"\"\"\n",
    "        # Word - document similarity\n",
    "        word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "        # Candidates - similarity\n",
    "        word_similarity = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "        # Extract the simliarest word's index\n",
    "        keywords_idx = [np.argmax(word_doc_similarity) // word_doc_similarity.shape[1]]\n",
    "\n",
    "        # Index list excluding the similarest index\n",
    "        candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "        # Repeat this for remaining counts (top_n - 1)\n",
    "        for _ in range(top_n - 1):\n",
    "            candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "            target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "            # Compute the MMR\n",
    "            mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "            mmr_idx = candidates_idx[np.argmax(mmr) // mmr.shape[1]]\n",
    "\n",
    "            # Update indices\n",
    "            keywords_idx.append(mmr_idx)\n",
    "            candidates_idx.remove(mmr_idx)\n",
    "\n",
    "        return [words[idx] for idx in keywords_idx]\n",
    "    \n",
    "\n",
    "    def BERT(self, bert_model, top_n):\n",
    "        \"\"\"\n",
    "            TODO: Do *BERT* model using CountVectorizer\n",
    "\n",
    "            :arg\n",
    "                bert_model(str): Sentence Transofmer name, see more information in https://huggingface.co/models?library=sentence-transformers\n",
    "                top_n(int): The number of high similarity words from candidates\n",
    "            :return\n",
    "                pandas.DataFrame: Movie index and {top_n} words with high similarity using cosine_similarity and MMR.\n",
    "        \"\"\"\n",
    "        num = len(self.dialog_df['movieid'])\n",
    "\n",
    "        n_gram_range = (3, 3)\n",
    "        \n",
    "        df = pd.DataFrame(columns=['movieid', 'Words(cosine_similarity)', 'Words(MMR)'])\n",
    "\n",
    "        for i in range(num):\n",
    "            print(f'Progress: {i + 1} of {num}')\n",
    "            count = CountVectorizer(ngram_range=n_gram_range, stop_words='english').fit(self.dialog_df.iloc[i, :].values)\n",
    "            candidates = count.get_feature_names_out()\n",
    "\n",
    "            model = SentenceTransformer(bert_model)\n",
    "\n",
    "            doc_embedding = model.encode(self.dialog_df.iloc[i, :].values)\n",
    "            candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "            (csim, mmr) = (self.__max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, 20),\n",
    "                self.__mmr(doc_embedding, candidate_embeddings, candidates, top_n, 0.7))\n",
    "\n",
    "            newrow = pd.DataFrame({\n",
    "                'movieid': [self.dialog_df.iloc[i, 0]],\n",
    "                'Words(cosine_similarity)': [csim],\n",
    "                'Words(MMR)': [mmr]},\n",
    "                columns=df.columns)\n",
    "            df = pd.concat([df, newrow], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def similarity(X, Y):\n",
    "        \"\"\"\n",
    "        TODO: Compute the cosine simliarity between X and Y. For avoiding the DivByZero, the denominator has 1e-7 minimum value.\n",
    "\n",
    "            :arg\n",
    "                X (numpy.ndarray): X data array\n",
    "                Y (numpy.ndarray): Y data array\n",
    "            :return\n",
    "                float\n",
    "        \"\"\"\n",
    "        return np.dot(X, Y) / ((norm(X) * norm(Y)) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "Import dataset, describe it briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brief information:\n",
      "Length of train data: 10006\n",
      "Length of test data: 1342\n",
      "\n",
      "Data information:\n",
      "Key parameters: ['movieMentions', 'respondentQuestions', 'messages', 'conversationId', 'respondentWorkerId', 'initiatorWorkerId', 'initiatorQuestions']\n",
      "Key parameters in Questions: ['suggested', 'seen', 'liked']\n",
      "Key parameters in messages: ['timeOffset', 'text', 'senderWorkerId', 'messageId']\n",
      "\n",
      "Context information:\n",
      "Total mentioned movie number (train): 52918\n",
      "Total mentioned movie number in unique (train): 6223\n",
      "Total message number (train): 182150\n",
      "Total mentioned movie number (test): 7154\n",
      "Total mentioned movie number in unique (test): 2007\n",
      "Total message number (test): 23952\n",
      "Average mentioned movie numbers per conversation (train): 5.288626823905656\n",
      "Average message numbers per conversation (train): 18.20407755346792\n",
      "Average mentioned movie numbers per conversation (test): 5.330849478390462\n",
      "Average message numbers per conversation (test): 17.847988077496275\n",
      "\n",
      "length of train dataset: 10006\n"
     ]
    }
   ],
   "source": [
    "parser = RedialParser('../dataset')\n",
    "parser.describe()  # Describe read dataset\n",
    "\n",
    "# Size of train data\n",
    "num = len(parser.train)\n",
    "print(f'length of train dataset: {num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Clear the special character and extract the text and movie indices\n",
    "- example: \"I like animations like @84779 and @191602\" → [i like animations like  and ], [84779, 191602]\n",
    "\n",
    "\n",
    "Specific:\n",
    "* Transform dataset structure.\n",
    "    * Original: [movieMentions, {messages}, conversationId, ...]\n",
    "    * Transformed: [movie_indices], [message_contexts], [[1st_movie_index], [2nd_...], ...], [[1st_message_context], [2nd_...], ...]\n",
    "    * Dialog Dataframe (*self.dialog_df*): {'movie_id': '1st message' + '2nd message' + ...} - Used in generation of **TF-IDF** matrix\n",
    "* Recognize movie indices\n",
    "    * **@** recognition: use re library's *findall(@\\d+)* function, it only detects '@' + index strings.\n",
    "* Clean up meaningless values\n",
    "    * Special characters: use re library's format *\\w+*, it only receives widechar characters.\n",
    "    * Movie index: remove context of them by using text slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieid</th>\n",
       "      <th>dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84779</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191602</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122159</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165710</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151313</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64456</th>\n",
       "      <td>204974</td>\n",
       "      <td>what type of movies do you like  hi i m looki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64457</th>\n",
       "      <td>85036</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64458</th>\n",
       "      <td>170277</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64459</th>\n",
       "      <td>149938</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64460</th>\n",
       "      <td>200018</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieid                                             dialog\n",
       "0       84779   hi there how are you i m looking for movie re...\n",
       "1      191602   hi there how are you i m looking for movie re...\n",
       "2      122159   hi there how are you i m looking for movie re...\n",
       "3      165710   hi there how are you i m looking for movie re...\n",
       "4      151313   hi there how are you i m looking for movie re...\n",
       "...       ...                                                ...\n",
       "64456  204974   what type of movies do you like  hi i m looki...\n",
       "64457   85036   hello  hi how can i help you so some of the m...\n",
       "64458  170277   hello  hi how can i help you so some of the m...\n",
       "64459  149938   hello  hi how can i help you so some of the m...\n",
       "64460  200018   hello  hi how can i help you so some of the m...\n",
       "\n",
       "[64461 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.preprocessing()\n",
    "parser.dialog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    \"\"\"\n",
    "    TODO: Minimize redundancy and maximize diversity - Get {top_n} keyword(s) using Cosine Similarity\n",
    "\n",
    "        :arg\n",
    "            doc_embedding(numpy.ndarray): Document embedding (Obtained from dialog)\n",
    "            candidate_embeddings(numpy.ndarray): Candidates embedding (Obtained from CountVectorizer)\n",
    "            words(numpy.ndarray): Candidates\n",
    "            top_n(int): Maximum size of high similarity indices\n",
    "            nr_candidates(int): Nearby Candidates, the word list has maximum size is this.\n",
    "        :return\n",
    "            list: list of high similarity keywords with length = {top_n}\n",
    "    \"\"\"\n",
    "    # Compute similarity of document and candidates\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # Compute similarity of candidates\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "    # Explict {top_n} words\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [words[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Compute the combination of keywords having minimum redundancy\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]\n",
    "\n",
    "\n",
    "def fmmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "    \"\"\"\n",
    "    TODO: Minimize redundancy and maximize diversity - Get {top_n} keyword(s) using MMR\n",
    "\n",
    "        :arg\n",
    "            doc_embedding(numpy.ndarray): Document embedding (Obtained from dialog)\n",
    "            candidate_embeddings(numpy.ndarray): Candidates embedding (Obtained from CountVectorizer)\n",
    "            words(numpy.ndarray): Candidates\n",
    "            top_n(int): Maximum size of high similarity indices\n",
    "            diversity(float): Diversity of candidates, higher diversity has larger candidates num.\n",
    "        :return\n",
    "            list: list of high similarity keywords with length = {top_n}\n",
    "    \"\"\"\n",
    "    # Word - document similarity\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # Candidates - similarity\n",
    "    word_similarity = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "    # Extract the simliarest word's index\n",
    "    keywords_idx = [np.argmax(word_doc_similarity) // word_doc_similarity.shape[1]]\n",
    "\n",
    "    # Index list excluding the similarest index\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # Repeat this for remaining counts (top_n - 1)\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Compute the MMR\n",
    "        mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr) // mmr.shape[1]]\n",
    "\n",
    "        # Update indices\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n",
    "\n",
    "\n",
    "def do_bert(model_name, id, value, top_n, df):\n",
    "    count = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit([value])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    doc_embedding = model.encode([value])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    (csim, mmr) = (max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, 20),\n",
    "        fmmr(doc_embedding, candidate_embeddings, candidates, top_n, 0.7))\n",
    "    \n",
    "    df.at[id, 'Words(cosine_similarity)'] = csim\n",
    "    df.at[id, 'Words(MMR)'] = mmr\n",
    "\n",
    "\n",
    "def BERT(bert_model, top_n):\n",
    "    \"\"\"\n",
    "    TODO: Do *BERT* model using CountVectorizer\n",
    "\n",
    "        :arg\n",
    "            bert_model(str): Sentence Transofmer name, see more information in https://huggingface.co/models?library=sentence-transformers\n",
    "            top_n(int): The number of high similarity words from candidates\n",
    "        :return\n",
    "            pandas.DataFrame: Movie index and {top_n} words with high similarity using cosine_similarity and MMR.\n",
    "    \"\"\"\n",
    "    num = len(parser.dialog_df['movieid'])\n",
    "    \n",
    "    df = pd.DataFrame({'movieid': parser.dialog_df['movieid'].values}, columns=['movieid', 'Words(cosine_similarity)', 'Words(MMR)'])\n",
    "    \n",
    "    from threading import Thread\n",
    "\n",
    "    max_thread = 20\n",
    "\n",
    "    threads = []\n",
    "    i = 0\n",
    "    txt = ''\n",
    "    while True:\n",
    "        remove_list = []\n",
    "        for th in threads:\n",
    "            if th.is_alive() is False:\n",
    "                remove_list.append(th)\n",
    "        \n",
    "        for th in remove_list:\n",
    "            threads.remove(th)\n",
    "        \n",
    "        if len(threads) >= max_thread:\n",
    "            continue\n",
    "\n",
    "        if i >= num:\n",
    "            break\n",
    "        \n",
    "        remove = '\\b' * len(txt)\n",
    "        txt = f'Progress: {round((i + 1) / num, 2)}% - {i + 1} of {num}\\n'\n",
    "        print(remove, end=txt)\n",
    "        p = Thread(target=do_bert,\n",
    "            args=(bert_model, parser.dialog_df.iloc[i, 0], parser.dialog_df.iloc[i, 1], top_n, df),\n",
    "            daemon=True)\n",
    "        threads.append(p)\n",
    "        p.start()\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0% - 1 of 64461\n",
      "Progress: 0.0% - 2 of 64461\n",
      "Progress: 0.0% - 3 of 64461\n",
      "Progress: 0.0% - 4 of 64461\n",
      "Progress: 0.0% - 5 of 64461\n",
      "Progress: 0.0% - 6 of 64461\n",
      "Progress: 0.0% - 7 of 64461\n",
      "Progress: 0.0% - 8 of 64461\n",
      "Progress: 0.0% - 9 of 64461\n",
      "Progress: 0.0% - 10 of 64461\n",
      "\bProgress: 0.0% - 11 of 64461\n",
      "\bProgress: 0.0% - 12 of 64461\n",
      "\bProgress: 0.0% - 13 of 64461\n",
      "\bProgress: 0.0% - 14 of 64461\n",
      "\bProgress: 0.0% - 15 of 64461\n",
      "\bProgress: 0.0% - 16 of 64461\n",
      "\bProgress: 0.0% - 17 of 64461\n",
      "\bProgress: 0.0% - 18 of 64461\n",
      "\bProgress: 0.0% - 19 of 64461\n",
      "\bProgress: 0.0% - 20 of 64461\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bert_df \u001b[39m=\u001b[39m BERT(\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-nli-mean-tokens\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m bert_df\n",
      "Cell \u001b[1;32mIn [111], line 115\u001b[0m, in \u001b[0;36mBERT\u001b[1;34m(bert_model, top_n)\u001b[0m\n\u001b[0;32m    113\u001b[0m txt \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     remove_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    116\u001b[0m     \u001b[39mfor\u001b[39;00m th \u001b[39min\u001b[39;00m threads:\n\u001b[0;32m    117\u001b[0m         \u001b[39mif\u001b[39;00m th\u001b[39m.\u001b[39mis_alive() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_df = BERT('distilbert-base-nli-mean-tokens', 5)\n",
    "bert_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "* 1. Extract words and their counts related to the movies. (Did not used, only for eye inspection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag words words related with movie genres\n",
    "mv_tags = ['comedy','scary','love','animation','artistic','war','sci','blood','hero','romantic','action']\n",
    "frequency = parser.get_frequency_matrix(mv_tags)\n",
    "frequency.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. Normal TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat, tfidf_columns = parser.get_tfidf_matrix(stop_words='english', min_df=0.2)\n",
    "\n",
    "# Construct dataset with id + word vectors\n",
    "cdata = np.concatenate((parser.dialog_df['movieid'].to_numpy().reshape(len(parser.dialog_df['dialog']), 1), tfidf_mat), axis=1)\n",
    "df_mv_tfidf = pd.DataFrame(cdata, columns=['id'] + tfidf_columns.tolist())\n",
    "df_mv_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = parser.BERT('distilbert-base-nli-mean-tokens', 5)\n",
    "bert_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Metrics\n",
    "* Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation function\n",
    "* param:\n",
    "    * data: array, vector space of texts.\n",
    "    * mv: target movie's index\n",
    "    * length: maximum length of recommendation\n",
    "        * default: 5\n",
    "    * simf: consine similarity function\n",
    "        * default: dot(X, y) / (normalize(X) * normalize(Y) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the consine similarity function's denominator has 1e-7 minimum value to avoid the divbyzero.\n",
    "def recommend(matrix, index, length=5, simf=lambda X, Y: np.dot(X, Y) / (1e-7 + norm(X) * norm(Y))):\n",
    "    sim = []\n",
    "    return sim\n",
    "\n",
    "    if df.loc[df['movieid'] == mv].empty:\n",
    "        return sim\n",
    "    \n",
    "    idx = df[df['movieid'] == mv].index.values[0]\n",
    "\n",
    "    for idx, data in enumerate(matrix):\n",
    "        if idx != i:\n",
    "            sim.append((simf(data[i], data[idx]), df.loc[i]['movieid']))\n",
    "    \n",
    "    sim.sort()\n",
    "    sim.reverse()\n",
    "    return sim[:length]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
