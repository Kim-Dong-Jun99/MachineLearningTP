{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Dataset imports\n",
    "import json\n",
    "\n",
    "# For restoring the dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "# Text manipulations\n",
    "import re\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cosine similarity\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# File download (Golve)\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library installation\n",
    "* NLTK - Natural Language toolkit\n",
    "* NetworkX - Structure, Dynamics, and Functions of complex networks Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.8.8)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install nltk\n",
    "!python -m pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK: Library for NLP Process\n",
    "* Usage:\n",
    "    * nltk.corpus.**stopwords**: stopwords of specific language\n",
    "    * nltk.tokenize.**RegexpTokenizer, sent_tokenize, word_tokenize**: Tokenize the input sentences\n",
    "    * nltk.stem.**WordNetLemmatizer**: Lemmatize the word net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetworkX: Library for PageRank(TextRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redial Parser\n",
    "A separated library for parsing the redial dataset\n",
    "\n",
    "class **RedialParser**\n",
    "- Restore(): Restore train, test, and movie dataset to initial state\n",
    "   * return:\n",
    "        * None\n",
    "- Movies(train): Get movie list in dataset\n",
    "   * param:\n",
    "        * train (bool): Target dataset, (train=True, test=False, all=None)\n",
    "   * return:\n",
    "        * dict: {index, MovieName}\n",
    "- describe(): Describe its datasets\n",
    "   * return:\n",
    "        * None\n",
    "- train: Train data of ReDial.\n",
    "- test: Test data of ReDial.\n",
    "- movie: Movie mention counts for ReDial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    TODO: initialization function for dataset reads\n",
    "\n",
    "        :arg\n",
    "            path (str): Dataset path.\n",
    "        :return\n",
    "            tuple: (train, test, df_mention)\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    for line in open(f\"{path}/train_data.jsonl\", \"r\"):\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "    test_data = []\n",
    "    for line in open(f\"{path}/test_data.jsonl\", \"r\"):\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "    mention_dataframe = pd.read_csv(f\"{path}/movies_with_mentions.csv\")\n",
    "\n",
    "    return train_data, test_data, mention_dataframe\n",
    "\n",
    "\n",
    "class RedialParser:\n",
    "    def __init__(self, path):\n",
    "        self.train, self.test, self.movie = load_data(path)\n",
    "\n",
    "        self._global_movie_list = None  # list of all movie names (global movie name data)\n",
    "        self._global_msg_list = None  # list of whole lines (global line data)\n",
    "        self._local_movie_list = None  # list of movie names (local movie name data)\n",
    "        self._local_msg_list = None  # list of lines (local line data)\n",
    "\n",
    "        self.dialog_df = None  # Sum of dialogs for each movie indices\n",
    "\n",
    "        self.__train = deepcopy(self.train)\n",
    "        self.__test = deepcopy(self.test)\n",
    "        self.__movie = deepcopy(self.movie)\n",
    "\n",
    "        self.__model = None\n",
    "\n",
    "\n",
    "    def Restore(self):\n",
    "        \"\"\"\n",
    "        TODO: Restore train, test, and movie dataset to initial state\n",
    "        \"\"\"\n",
    "        self.train = deepcopy(self.__train)\n",
    "        self.test = deepcopy(self.__test)\n",
    "        self.movie = deepcopy(self.__movie)\n",
    "\n",
    "\n",
    "    def Movies(self, train=True) -> dict:\n",
    "        \"\"\"\n",
    "        TODO: Get movie list in dataset\n",
    "\n",
    "            :arg\n",
    "                train (bool): Target dataset, (train=True, test=False, all=None)\n",
    "            :return\n",
    "                dict: {index, MovieName}\n",
    "        \"\"\"\n",
    "        if train is None:\n",
    "            result = self.Movies()\n",
    "            result.update(self.Movies(False))\n",
    "            return result\n",
    "\n",
    "        target = None\n",
    "        if train is True:\n",
    "            target = self.train\n",
    "        elif train is False:\n",
    "            target = self.test\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if target is not None:\n",
    "            for elem in target:\n",
    "                result.update(elem['movieMentions'])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        TODO: Describe its datasets\n",
    "        \"\"\"\n",
    "        len1, len2 = len(self.train), len(self.test)\n",
    "        n1, n2 = 0, 0\n",
    "        m1, m2 = 0, 0\n",
    "\n",
    "        for e in self.train:\n",
    "            n1 += len(e['movieMentions'])\n",
    "            m1 += len(e['messages'])\n",
    "        for e in self.test:\n",
    "            n2 += len(e['movieMentions'])\n",
    "            m2 += len(e['messages'])\n",
    "\n",
    "        print('Brief information:\\n'\n",
    "              f'Length of train data: {len1}\\n'\n",
    "              f'Length of test data: {len2}\\n\\n'\n",
    "              'Data information:\\n'\n",
    "              f'Key parameters: {list(self.train[0].keys())}\\n'\n",
    "              f'Key parameters in Questions: {list(list(self.train[0][\"respondentQuestions\"].values())[0].keys())}\\n'\n",
    "              f'Key parameters in messages: {list(self.train[0][\"messages\"][0].keys())}\\n\\n'\n",
    "              'Context information:\\n'\n",
    "              f'Total mentioned movie number (train): {n1}\\n'\n",
    "              f'Total mentioned movie number in unique (train): {len(self.Movies())}\\n'\n",
    "              f'Total message number (train): {m1}\\n'\n",
    "              f'Total mentioned movie number (test): {n2}\\n'\n",
    "              f'Total mentioned movie number in unique (test): {len(self.Movies(False))}\\n'\n",
    "              f'Total message number (test): {m2}\\n'\n",
    "              f'Average mentioned movie numbers per conversation (train): {n1 / len1}\\n'\n",
    "              f'Average message numbers per conversation (train): {m1 / len1}\\n'\n",
    "              f'Average mentioned movie numbers per conversation (test): {n2 / len2}\\n'\n",
    "              f'Average message numbers per conversation (test): {m2 / len2}\\n\\n'\n",
    "              , end='')\n",
    "    \n",
    "\n",
    "    def preprocessing(self):\n",
    "        \"\"\"\n",
    "        TODO: Regroup train dataset into purposed structure and clean up data\n",
    "        \"\"\"\n",
    "        compile = re.compile(\"\\W+\")  # Format\n",
    "        \n",
    "        ran = range(len(self.train))\n",
    "\n",
    "        # initialize list\n",
    "        self._global_movie_list = []\n",
    "        self._global_msg_list = []\n",
    "        self._local_movie_list = [[] for _ in ran]\n",
    "        self._local_msg_list = [[] for _ in ran]\n",
    "\n",
    "        for i, data in enumerate(self.train):\n",
    "            for msg in data['messages']:  # append line to the lists\n",
    "                self._local_msg_list[i].append(msg['text'])\n",
    "                self._global_msg_list.append(msg['text'])\n",
    "\n",
    "            # Extract movie indices\n",
    "            for idx, line in enumerate(self._local_msg_list[i]):\n",
    "                numbers = re.findall(r'@\\d+', line)  # find number keywords (ex: @12345)\n",
    "                for number in numbers:\n",
    "                    self._local_movie_list[i].append(number[1:])\n",
    "                    self._global_movie_list.append(number[1:])\n",
    "\n",
    "                    # Remove index string\n",
    "                    pos = line.index(number)\n",
    "                    line = self._local_msg_list[i][idx] = line[0: pos] + line[pos + len(number): len(line)]\n",
    "\n",
    "                # Post: clear meaningless words\n",
    "                a = compile.sub(\" \", line)  # Clear special character\n",
    "                self._local_msg_list[i][idx] = a.lower()  # lower character\n",
    "\n",
    "        # Construct dialog dataframe\n",
    "        self.dialog_df = pd.DataFrame(columns=[\"movieid\", \"dialog\"])\n",
    "\n",
    "        for lines, movies in zip(self._local_msg_list, self._local_movie_list):\n",
    "            dig = ''\n",
    "            for line in lines:  # concatenate all sentences in related message dialog\n",
    "                dig += ' ' + str(line)\n",
    "            \n",
    "            for mv in movies:\n",
    "                newrow = pd.DataFrame({'movieid': [mv], 'dialog': [dig]}, columns=self.dialog_df.columns)\n",
    "                self.dialog_df = pd.concat([self.dialog_df, newrow], ignore_index=True)\n",
    "        \n",
    "        # Fill NaN with empty sentence\n",
    "        self.dialog_df['dialog'].fillna('', inplace=True)\n",
    "    \n",
    "\n",
    "    def get_frequency_matrix(self, tags):\n",
    "        \"\"\"\n",
    "        TODO: compute the frequency of tag words to obtain the TF-IDFs matrix\n",
    "\n",
    "            :arg\n",
    "                tags (list): list of key words.\n",
    "            :return\n",
    "                pandas.DataFrame: frequency matrix of tag words.\n",
    "        \"\"\"\n",
    "        stop_word_eng = set(stopwords.words('english'))\n",
    "        ran = range(len(self.train))\n",
    "\n",
    "        msg_list = deepcopy(self._local_msg_list)\n",
    "\n",
    "        for i in ran:\n",
    "            msg_list[i] = [j for j in msg_list[i] if j not in stop_word_eng]  # Clear stopwords\n",
    "\n",
    "        # Lemmatizer class\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "        # mv_tags = ['comedy','scary','love','animation','artistic','war','sci','blood','hero','romantic','action']\n",
    "        x = pd.DataFrame(columns=['id'] + tags)\n",
    "\n",
    "        for idx, msg in enumerate(msg_list):\n",
    "            result_pre_lem = [token.tokenize(j) for j in msg]\n",
    "            middle_pre_lem = [r for j in result_pre_lem for r in j]\n",
    "            final_lem = [lemmatizer.lemmatize(j) for j in middle_pre_lem if not j in stop_word_eng]  # Remove stopword\n",
    "\n",
    "            # Lemmatization\n",
    "            english = pd.Series(final_lem)\n",
    "            for word in english:\n",
    "                if word in tags:\n",
    "                    for movie in self._local_movie_list[idx]:\n",
    "                        if x[x['id'] == movie].empty:\n",
    "                            new_row = pd.DataFrame({'id': [movie]}, columns=x.columns)\n",
    "                            x = pd.concat([x, new_row], ignore_index=True)\n",
    "                            x.fillna(0, inplace=True)\n",
    "                        x.loc[x['id'] == movie, word] += 1\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_tfidf_matrix(self, **tfidf_keys):\n",
    "        \"\"\"\n",
    "        TODO: Compute TF-IDFs matrix\n",
    "\n",
    "            :arg\n",
    "                tfidf_keys(keyword dict): TfidfVectorizer parameters\n",
    "            :return\n",
    "                numpy.ndrarry: TF-IDFs matrix\n",
    "                numpy.ndarray: feature name of TF-IDFs (word)\n",
    "        \"\"\"\n",
    "        # Vectorizer class\n",
    "        tfidf = TfidfVectorizer(**tfidf_keys)  # Ignore English Stopwords\n",
    "\n",
    "        # Obtain matrix\n",
    "        tfidf_df = tfidf.fit_transform(self.dialog_df['dialog'])\n",
    "\n",
    "        return tfidf_df.toarray(), tfidf.get_feature_names_out()\n",
    "    \n",
    "\n",
    "    def get_textrank_matrix(self):\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "        length = len(self.dialog_df['movieid'])\n",
    "\n",
    "        def tokenization(sentences):\n",
    "            return [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "        def preprocess_sentence(sentence):\n",
    "            sentence = [re.sub(r'[^a-zA-z\\s]', '', word).lower() for word in sentence]\n",
    "            return [word for word in sentence if word not in stop_words and word]\n",
    "\n",
    "        def preprocess_sentences(sentences):\n",
    "            return [preprocess_sentence(sentence) for sentence in sentences]\n",
    "        \n",
    "        df = pd.DataFrame(columns=['text','sentences'])\n",
    "        df['text'] = self.dialog_df['dialog'].values\n",
    "        df['sentences'] = self.dialog_df['dialog'].values\n",
    "\n",
    "        df['sentences'] = df['sentences'].apply(sent_tokenize)\n",
    "        df['tokenized_sentences'] = df['sentences'].apply(tokenization)\n",
    "        df['tokenized_sentences'] = df['tokenized_sentences'].apply(preprocess_sentences)\n",
    "\n",
    "        embedding_dim = length ** 2\n",
    "        zero_vector = np.zeros(embedding_dim)\n",
    "\n",
    "        glove_dict = {}\n",
    "        f = open('../train/glove.6B.100d.txt', encoding=\"utf8\")  # 100차원의 GloVe 벡터를 사용\n",
    "\n",
    "        for line in f:\n",
    "            word_vector = line.split()\n",
    "            word = word_vector[0]\n",
    "            word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\n",
    "            glove_dict[word] = word_vector_arr\n",
    "        f.close()\n",
    "\n",
    "        # 단어 벡터의 평균으로부터 문장 벡터 반환\n",
    "        def calculate_sentence_vector(sentence):\n",
    "            if len(sentence) != 0:\n",
    "                return sum([glove_dict.get(word, zero_vector) for word in sentence]) / len(sentence)\n",
    "            else:\n",
    "                return zero_vector\n",
    "\n",
    "        # 각 문장에 대해서 문장 벡터를 반환\n",
    "        def sentences_to_vectors(sentences):\n",
    "            return [calculate_sentence_vector(sentence) for sentence in sentences]\n",
    "        \n",
    "        # 문장 벡터들 간의 코사인 유사도\n",
    "        def similarity_matrix(sentence_embedding):\n",
    "            sim_mat = np.zeros([len(sentence_embedding), len(sentence_embedding)])\n",
    "            for i in range(len(sentence_embedding)):\n",
    "                for j in range(len(sentence_embedding)):\n",
    "                    sim_mat[i][j] = cosine_similarity(sentence_embedding[i].reshape(1, embedding_dim),sentence_embedding[j].reshape(1, embedding_dim))[0,0]\n",
    "            return sim_mat\n",
    "\n",
    "        # 페이지랭크 알고리즘의 입력으로 사용하여 각 문장의 점수 반환\n",
    "        def calculate_score(sim_matrix):\n",
    "            nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "            scores = nx.pagerank(nx_graph)\n",
    "            return scores\n",
    "            \n",
    "        # 점수가 가장 높은 상위 3개의 문서의 요약문\n",
    "        def ranked_sentences(sentences, scores, n=3):\n",
    "            top_scores = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "            top_n_sentences = [sentence  for score,sentence in top_scores[:n]]\n",
    "            return \" \".join(top_n_sentences)\n",
    "        \n",
    "        df['SentenceEmbedding'] = df['tokenized_sentences'].apply(sentences_to_vectors)\n",
    "        df['SimMatrix'] = df['SentenceEmbedding'].apply(similarity_matrix)\n",
    "        df['score'] = df['SimMatrix'].apply(calculate_score)\n",
    "        df['summary'] = df.apply(lambda x: ranked_sentences(x.sentences, x.score), axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def similarity(X, Y):\n",
    "        \"\"\"\n",
    "        TODO: Compute the cosine simliarity between X and Y. For avoiding the DivByZero, the denominator has 1e-7 minimum value.\n",
    "\n",
    "            :arg\n",
    "                X (numpy.ndarray): X data array\n",
    "                Y (numpy.ndarray): Y data array\n",
    "            :return\n",
    "                float\n",
    "        \"\"\"\n",
    "        return np.dot(X, Y) / ((norm(X) * norm(Y)) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "Import dataset, describe it briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brief information:\n",
      "Length of train data: 10006\n",
      "Length of test data: 1342\n",
      "\n",
      "Data information:\n",
      "Key parameters: ['movieMentions', 'respondentQuestions', 'messages', 'conversationId', 'respondentWorkerId', 'initiatorWorkerId', 'initiatorQuestions']\n",
      "Key parameters in Questions: ['suggested', 'seen', 'liked']\n",
      "Key parameters in messages: ['timeOffset', 'text', 'senderWorkerId', 'messageId']\n",
      "\n",
      "Context information:\n",
      "Total mentioned movie number (train): 52918\n",
      "Total mentioned movie number in unique (train): 6223\n",
      "Total message number (train): 182150\n",
      "Total mentioned movie number (test): 7154\n",
      "Total mentioned movie number in unique (test): 2007\n",
      "Total message number (test): 23952\n",
      "Average mentioned movie numbers per conversation (train): 5.288626823905656\n",
      "Average message numbers per conversation (train): 18.20407755346792\n",
      "Average mentioned movie numbers per conversation (test): 5.330849478390462\n",
      "Average message numbers per conversation (test): 17.847988077496275\n",
      "\n",
      "length of train dataset: 10006\n"
     ]
    }
   ],
   "source": [
    "parser = RedialParser('../dataset')\n",
    "parser.describe()  # Describe read dataset\n",
    "\n",
    "# Size of train data\n",
    "num = len(parser.train)\n",
    "print(f'length of train dataset: {num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Clear the special character and extract the text and movie indices\n",
    "- example: \"I like animations like @84779 and @191602\" → [i like animations like  and ], [84779, 191602]\n",
    "\n",
    "\n",
    "Specific:\n",
    "* Transform dataset structure.\n",
    "    * Original: [movieMentions, {messages}, conversationId, ...]\n",
    "    * Transformed: [movie_indices], [message_contexts], [[1st_movie_index], [2nd_...], ...], [[1st_message_context], [2nd_...], ...]\n",
    "    * Dialog Dataframe (*self.dialog_df*): {'movie_id': '1st message' + '2nd message' + ...} - Used in generation of **TF-IDF** matrix\n",
    "* Recognize movie indices\n",
    "    * **@** recognition: use re library's *findall(@\\d+)* function, it only detects '@' + index strings.\n",
    "* Clean up meaningless values\n",
    "    * Special characters: use re library's format *\\w+*, it only receives widechar characters.\n",
    "    * Movie index: remove context of them by using text slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieid</th>\n",
       "      <th>dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84779</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191602</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122159</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165710</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151313</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64456</th>\n",
       "      <td>204974</td>\n",
       "      <td>what type of movies do you like  hi i m looki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64457</th>\n",
       "      <td>85036</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64458</th>\n",
       "      <td>170277</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64459</th>\n",
       "      <td>149938</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64460</th>\n",
       "      <td>200018</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieid                                             dialog\n",
       "0       84779   hi there how are you i m looking for movie re...\n",
       "1      191602   hi there how are you i m looking for movie re...\n",
       "2      122159   hi there how are you i m looking for movie re...\n",
       "3      165710   hi there how are you i m looking for movie re...\n",
       "4      151313   hi there how are you i m looking for movie re...\n",
       "...       ...                                                ...\n",
       "64456  204974   what type of movies do you like  hi i m looki...\n",
       "64457   85036   hello  hi how can i help you so some of the m...\n",
       "64458  170277   hello  hi how can i help you so some of the m...\n",
       "64459  149938   hello  hi how can i help you so some of the m...\n",
       "64460  200018   hello  hi how can i help you so some of the m...\n",
       "\n",
       "[64461 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.preprocessing()\n",
    "parser.dialog_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "* 1. Extract words and their counts related to the movies. (Did not used, only for eye inspection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comedy</th>\n",
       "      <th>scary</th>\n",
       "      <th>love</th>\n",
       "      <th>animation</th>\n",
       "      <th>artistic</th>\n",
       "      <th>war</th>\n",
       "      <th>sci</th>\n",
       "      <th>blood</th>\n",
       "      <th>hero</th>\n",
       "      <th>romantic</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "      <td>5101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.840227</td>\n",
       "      <td>1.325230</td>\n",
       "      <td>7.919035</td>\n",
       "      <td>0.204666</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.583023</td>\n",
       "      <td>0.990982</td>\n",
       "      <td>0.059792</td>\n",
       "      <td>0.304058</td>\n",
       "      <td>1.089982</td>\n",
       "      <td>3.916095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.739531</td>\n",
       "      <td>10.106137</td>\n",
       "      <td>20.996916</td>\n",
       "      <td>1.584999</td>\n",
       "      <td>0.091594</td>\n",
       "      <td>4.267986</td>\n",
       "      <td>6.156864</td>\n",
       "      <td>0.447819</td>\n",
       "      <td>2.396133</td>\n",
       "      <td>6.209539</td>\n",
       "      <td>19.184717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>449.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>673.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            comedy        scary         love    animation     artistic  \\\n",
       "count  5101.000000  5101.000000  5101.000000  5101.000000  5101.000000   \n",
       "mean      5.840227     1.325230     7.919035     0.204666     0.006469   \n",
       "std      21.739531    10.106137    20.996916     1.584999     0.091594   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     2.000000     0.000000     0.000000   \n",
       "75%       3.000000     0.000000     6.000000     0.000000     0.000000   \n",
       "max     449.000000   494.000000   380.000000    64.000000     2.000000   \n",
       "\n",
       "               war          sci        blood         hero     romantic  \\\n",
       "count  5101.000000  5101.000000  5101.000000  5101.000000  5101.000000   \n",
       "mean      0.583023     0.990982     0.059792     0.304058     1.089982   \n",
       "std       4.267986     6.156864     0.447819     2.396133     6.209539   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max     175.000000   160.000000    15.000000    81.000000   153.000000   \n",
       "\n",
       "            action  \n",
       "count  5101.000000  \n",
       "mean      3.916095  \n",
       "std      19.184717  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       2.000000  \n",
       "max     673.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag words words related with movie genres\n",
    "mv_tags = ['comedy','scary','love','animation','artistic','war','sci','blood','hero','romantic','action']\n",
    "frequency = parser.get_frequency_matrix(mv_tags)\n",
    "frequency.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. Normal TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bye</th>\n",
       "      <th>check</th>\n",
       "      <th>comedy</th>\n",
       "      <th>day</th>\n",
       "      <th>did</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>funny</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>...</th>\n",
       "      <th>saw</th>\n",
       "      <th>seen</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>sure</th>\n",
       "      <th>thank</th>\n",
       "      <th>thanks</th>\n",
       "      <th>think</th>\n",
       "      <th>ve</th>\n",
       "      <th>watch</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84779</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191602</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122159</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165710</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151313</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.234526</td>\n",
       "      <td>0.20677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64456</th>\n",
       "      <td>204974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182008</td>\n",
       "      <td>0.085112</td>\n",
       "      <td>0.297298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64457</th>\n",
       "      <td>85036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253538</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.178612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64458</th>\n",
       "      <td>170277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253538</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.178612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64459</th>\n",
       "      <td>149938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253538</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.178612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64460</th>\n",
       "      <td>200018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170075</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253538</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.178612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64461 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       bye check    comedy     day       did     enjoy     funny  \\\n",
       "0       84779  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "1      191602  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "2      122159  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "3      165710  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "4      151313  0.155447   0.0  0.232591     0.0       0.0  0.226592  0.234526   \n",
       "...       ...       ...   ...       ...     ...       ...       ...       ...   \n",
       "64456  204974       0.0   0.0       0.0     0.0  0.173777       0.0       0.0   \n",
       "64457   85036       0.0   0.0       0.0  0.4115       0.0  0.248506       0.0   \n",
       "64458  170277       0.0   0.0       0.0  0.4115       0.0  0.248506       0.0   \n",
       "64459  149938       0.0   0.0       0.0  0.4115       0.0  0.248506       0.0   \n",
       "64460  200018       0.0   0.0       0.0  0.4115       0.0  0.248506       0.0   \n",
       "\n",
       "           good     great  ...       saw      seen suggestions      sure  \\\n",
       "0       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "1       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "2       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "3       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "4       0.20677       0.0  ...       0.0       0.0         0.0       0.0   \n",
       "...         ...       ...  ...       ...       ...         ...       ...   \n",
       "64456  0.242174       0.0  ...  0.182008  0.085112    0.297298       0.0   \n",
       "64457  0.170075  0.070963  ...  0.127822  0.059773         0.0  0.253538   \n",
       "64458  0.170075  0.070963  ...  0.127822  0.059773         0.0  0.253538   \n",
       "64459  0.170075  0.070963  ...  0.127822  0.059773         0.0  0.253538   \n",
       "64460  0.170075  0.070963  ...  0.127822  0.059773         0.0  0.253538   \n",
       "\n",
       "          thank    thanks     think   ve     watch       yes  \n",
       "0           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "1           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "2           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "3           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "4           0.0  0.147461       0.0  0.0       0.0       0.0  \n",
       "...         ...       ...       ...  ...       ...       ...  \n",
       "64456       0.0   0.11514       0.0  0.0  0.139028       0.0  \n",
       "64457  0.290231       0.0  0.304353  0.0  0.292913  0.178612  \n",
       "64458  0.290231       0.0  0.304353  0.0  0.292913  0.178612  \n",
       "64459  0.290231       0.0  0.304353  0.0  0.292913  0.178612  \n",
       "64460  0.290231       0.0  0.304353  0.0  0.292913  0.178612  \n",
       "\n",
       "[64461 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat, tfidf_columns = parser.get_tfidf_matrix(stop_words='english', min_df=0.2)\n",
    "\n",
    "# Construct dataset with id + word vectors\n",
    "cdata = np.concatenate((parser.dialog_df['movieid'].to_numpy().reshape(len(parser.dialog_df['dialog']), 1), tfidf_mat), axis=1)\n",
    "df_mv_tfidf = pd.DataFrame(cdata, columns=['id'] + tfidf_columns.tolist())\n",
    "df_mv_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3. TextRank TF-IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenized_sentences</th>\n",
       "      <th>SentenceEmbedding</th>\n",
       "      <th>SimMatrix</th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "      <td>[ hi there how are you i m looking for movie r...</td>\n",
       "      <td>[[hi, looking, movie, recommendations, okay, k...</td>\n",
       "      <td>[[-0.056827486, 0.2443628, 0.3708559, -0.29126...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "      <td>[ hi there how are you i m looking for movie r...</td>\n",
       "      <td>[[hi, looking, movie, recommendations, okay, k...</td>\n",
       "      <td>[[-0.056827486, 0.2443628, 0.3708559, -0.29126...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "      <td>[ hi there how are you i m looking for movie r...</td>\n",
       "      <td>[[hi, looking, movie, recommendations, okay, k...</td>\n",
       "      <td>[[-0.056827486, 0.2443628, 0.3708559, -0.29126...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "      <td>[ hi there how are you i m looking for movie r...</td>\n",
       "      <td>[[hi, looking, movie, recommendations, okay, k...</td>\n",
       "      <td>[[-0.056827486, 0.2443628, 0.3708559, -0.29126...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "      <td>[ hi there how are you i m looking for movie r...</td>\n",
       "      <td>[[hi, looking, movie, recommendations, okay, k...</td>\n",
       "      <td>[[-0.056827486, 0.2443628, 0.3708559, -0.29126...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hi there how are you i m looking for movie re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64456</th>\n",
       "      <td>what type of movies do you like  hi i m looki...</td>\n",
       "      <td>[ what type of movies do you like  hi i m look...</td>\n",
       "      <td>[[type, movies, like, hi, looking, movie, sugg...</td>\n",
       "      <td>[[-0.027324153, 0.29030323, 0.4507456, -0.3851...</td>\n",
       "      <td>[[0.9999999403953552]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>what type of movies do you like  hi i m looki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64457</th>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "      <td>[ hello  hi how can i help you so some of the ...</td>\n",
       "      <td>[[hello, hi, help, movies, really, enjoy, thou...</td>\n",
       "      <td>[[-0.16118895274332978, 0.28092597955837845, 0...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64458</th>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "      <td>[ hello  hi how can i help you so some of the ...</td>\n",
       "      <td>[[hello, hi, help, movies, really, enjoy, thou...</td>\n",
       "      <td>[[-0.16118895274332978, 0.28092597955837845, 0...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64459</th>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "      <td>[ hello  hi how can i help you so some of the ...</td>\n",
       "      <td>[[hello, hi, help, movies, really, enjoy, thou...</td>\n",
       "      <td>[[-0.16118895274332978, 0.28092597955837845, 0...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64460</th>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "      <td>[ hello  hi how can i help you so some of the ...</td>\n",
       "      <td>[[hello, hi, help, movies, really, enjoy, thou...</td>\n",
       "      <td>[[-0.16118895274332978, 0.28092597955837845, 0...</td>\n",
       "      <td>[[1.0]]</td>\n",
       "      <td>{0: 1.0}</td>\n",
       "      <td>hello  hi how can i help you so some of the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64461 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0       hi there how are you i m looking for movie re...   \n",
       "1       hi there how are you i m looking for movie re...   \n",
       "2       hi there how are you i m looking for movie re...   \n",
       "3       hi there how are you i m looking for movie re...   \n",
       "4       hi there how are you i m looking for movie re...   \n",
       "...                                                  ...   \n",
       "64456   what type of movies do you like  hi i m looki...   \n",
       "64457   hello  hi how can i help you so some of the m...   \n",
       "64458   hello  hi how can i help you so some of the m...   \n",
       "64459   hello  hi how can i help you so some of the m...   \n",
       "64460   hello  hi how can i help you so some of the m...   \n",
       "\n",
       "                                               sentences  \\\n",
       "0      [ hi there how are you i m looking for movie r...   \n",
       "1      [ hi there how are you i m looking for movie r...   \n",
       "2      [ hi there how are you i m looking for movie r...   \n",
       "3      [ hi there how are you i m looking for movie r...   \n",
       "4      [ hi there how are you i m looking for movie r...   \n",
       "...                                                  ...   \n",
       "64456  [ what type of movies do you like  hi i m look...   \n",
       "64457  [ hello  hi how can i help you so some of the ...   \n",
       "64458  [ hello  hi how can i help you so some of the ...   \n",
       "64459  [ hello  hi how can i help you so some of the ...   \n",
       "64460  [ hello  hi how can i help you so some of the ...   \n",
       "\n",
       "                                     tokenized_sentences  \\\n",
       "0      [[hi, looking, movie, recommendations, okay, k...   \n",
       "1      [[hi, looking, movie, recommendations, okay, k...   \n",
       "2      [[hi, looking, movie, recommendations, okay, k...   \n",
       "3      [[hi, looking, movie, recommendations, okay, k...   \n",
       "4      [[hi, looking, movie, recommendations, okay, k...   \n",
       "...                                                  ...   \n",
       "64456  [[type, movies, like, hi, looking, movie, sugg...   \n",
       "64457  [[hello, hi, help, movies, really, enjoy, thou...   \n",
       "64458  [[hello, hi, help, movies, really, enjoy, thou...   \n",
       "64459  [[hello, hi, help, movies, really, enjoy, thou...   \n",
       "64460  [[hello, hi, help, movies, really, enjoy, thou...   \n",
       "\n",
       "                                       SentenceEmbedding  \\\n",
       "0      [[-0.056827486, 0.2443628, 0.3708559, -0.29126...   \n",
       "1      [[-0.056827486, 0.2443628, 0.3708559, -0.29126...   \n",
       "2      [[-0.056827486, 0.2443628, 0.3708559, -0.29126...   \n",
       "3      [[-0.056827486, 0.2443628, 0.3708559, -0.29126...   \n",
       "4      [[-0.056827486, 0.2443628, 0.3708559, -0.29126...   \n",
       "...                                                  ...   \n",
       "64456  [[-0.027324153, 0.29030323, 0.4507456, -0.3851...   \n",
       "64457  [[-0.16118895274332978, 0.28092597955837845, 0...   \n",
       "64458  [[-0.16118895274332978, 0.28092597955837845, 0...   \n",
       "64459  [[-0.16118895274332978, 0.28092597955837845, 0...   \n",
       "64460  [[-0.16118895274332978, 0.28092597955837845, 0...   \n",
       "\n",
       "                    SimMatrix     score  \\\n",
       "0                     [[1.0]]  {0: 1.0}   \n",
       "1                     [[1.0]]  {0: 1.0}   \n",
       "2                     [[1.0]]  {0: 1.0}   \n",
       "3                     [[1.0]]  {0: 1.0}   \n",
       "4                     [[1.0]]  {0: 1.0}   \n",
       "...                       ...       ...   \n",
       "64456  [[0.9999999403953552]]  {0: 1.0}   \n",
       "64457                 [[1.0]]  {0: 1.0}   \n",
       "64458                 [[1.0]]  {0: 1.0}   \n",
       "64459                 [[1.0]]  {0: 1.0}   \n",
       "64460                 [[1.0]]  {0: 1.0}   \n",
       "\n",
       "                                                 summary  \n",
       "0       hi there how are you i m looking for movie re...  \n",
       "1       hi there how are you i m looking for movie re...  \n",
       "2       hi there how are you i m looking for movie re...  \n",
       "3       hi there how are you i m looking for movie re...  \n",
       "4       hi there how are you i m looking for movie re...  \n",
       "...                                                  ...  \n",
       "64456   what type of movies do you like  hi i m looki...  \n",
       "64457   hello  hi how can i help you so some of the m...  \n",
       "64458   hello  hi how can i help you so some of the m...  \n",
       "64459   hello  hi how can i help you so some of the m...  \n",
       "64460   hello  hi how can i help you so some of the m...  \n",
       "\n",
       "[64461 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = parser.get_textrank_matrix()\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Metrics\n",
    "* Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the consine similarity function's denominator has 1e-7 minimum value to avoid the divbyzero.\n",
    "c_sim = lambda X, Y: np.dot(X, Y) / (1e-7 + norm(X) * norm(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation function\n",
    "* param:\n",
    "    * data: array, vector space of texts.\n",
    "    * mv: target movie's index\n",
    "    * length: maximum length of recommendation\n",
    "        * default: 5\n",
    "    * simf: consine similarity function\n",
    "        * default: dot(X, y) / (normalize(X) * normalize(Y) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(matrix, index, length=5, simf=c_sim):\n",
    "    sim = []\n",
    "\n",
    "    for idx, data in enumerate(matrix):\n",
    "        if idx != index:\n",
    "            sim.append(simf(data[index], data[idx]), index)\n",
    "    \n",
    "    sim.sort()\n",
    "    sim.reverse()\n",
    "    return sim[:length]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
