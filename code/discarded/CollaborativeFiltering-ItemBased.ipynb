{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redial Parser\n",
    "A separated library for parsing the redial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: redial_parser: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir redial_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class **RedialParser**\n",
    "- Restore(): Restore train, test, and movie dataset to initial state\n",
    "   * return:\n",
    "        * None\n",
    "- Movies(train): Get movie list in dataset\n",
    "   * param:\n",
    "        * train (bool): Target dataset, (train=True, test=False, all=None)\n",
    "   * return:\n",
    "        * dict: {index, MovieName}\n",
    "- describe(): Describe its datasets\n",
    "   * return:\n",
    "        * None\n",
    "- train: Train data of ReDial.\n",
    "- test: Test data of ReDial.\n",
    "- movie: Movie mention counts for ReDial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting redial_parser/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile redial_parser/__init__.py\n",
    "# Note: Write down the parser class in the separated file, redial_parser.\n",
    "# Since we want the word2vec notebook to contain the context of parser, we put the writefile to do that.\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import pandas\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    TODO: initialization function for dataset reads\n",
    "\n",
    "        :arg\n",
    "            path (str): Dataset path.\n",
    "        :return\n",
    "            tuple: (train, test, df_mention)\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    for line in open(f\"{path}/train_data.jsonl\", \"r\"):\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "    test_data = []\n",
    "    for line in open(f\"{path}/test_data.jsonl\", \"r\"):\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "    mention_dataframe = pandas.read_csv(f\"{path}/movies_with_mentions.csv\")\n",
    "\n",
    "    return train_data, test_data, mention_dataframe\n",
    "\n",
    "\n",
    "\n",
    "class RedialParser:\n",
    "    def __init__(self, path):\n",
    "        self.train, self.test, self.movie = load_data(path)\n",
    "\n",
    "        self.__train = deepcopy(self.train)\n",
    "        self.__test = deepcopy(self.test)\n",
    "        self.__movie = deepcopy(self.movie)\n",
    "\n",
    "        self.__model = None\n",
    "\n",
    "    def Restore(self):\n",
    "        \"\"\"\n",
    "        TODO: Restore train, test, and movie dataset to initial state\n",
    "        \"\"\"\n",
    "        self.train = deepcopy(self.__train)\n",
    "        self.test = deepcopy(self.__test)\n",
    "        self.movie = deepcopy(self.__movie)\n",
    "\n",
    "    def Movies(self, train=True) -> dict:\n",
    "        \"\"\"\n",
    "        TODO: Get movie list in dataset\n",
    "\n",
    "            :arg\n",
    "                train (bool): Target dataset, (train=True, test=False, all=None)\n",
    "            :return\n",
    "                dict: {index, MovieName}\n",
    "        \"\"\"\n",
    "        if train is None:\n",
    "            result = self.Movies()\n",
    "            result.update(self.Movies(False))\n",
    "            return result\n",
    "\n",
    "        target = None\n",
    "        if train is True:\n",
    "            target = self.train\n",
    "        elif train is False:\n",
    "            target = self.test\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if target is not None:\n",
    "            for elem in target:\n",
    "                result.update(elem['movieMentions'])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        TODO: Describe its datasets\n",
    "        \"\"\"\n",
    "        len1, len2 = len(self.train), len(self.test)\n",
    "        n1, n2 = 0, 0\n",
    "        m1, m2 = 0, 0\n",
    "\n",
    "        for e in self.train:\n",
    "            n1 += len(e['movieMentions'])\n",
    "            m1 += len(e['messages'])\n",
    "        for e in self.test:\n",
    "            n2 += len(e['movieMentions'])\n",
    "            m2 += len(e['messages'])\n",
    "\n",
    "        print('Brief information:\\n'\n",
    "              f'Length of train data: {len1}\\n'\n",
    "              f'Length of test data: {len2}\\n\\n'\n",
    "              'Data information:\\n'\n",
    "              f'Key parameters: {list(self.train[0].keys())}\\n'\n",
    "              f'Key parameters in Questions: {list(list(self.train[0][\"respondentQuestions\"].values())[0].keys())}\\n'\n",
    "              f'Key parameters in messages: {list(self.train[0][\"messages\"][0].keys())}\\n\\n'\n",
    "              'Context information:\\n'\n",
    "              f'Total mentioned movie number (train): {n1}\\n'\n",
    "              f'Total mentioned movie number in unique (train): {len(self.Movies())}\\n'\n",
    "              f'Total message number (train): {m1}\\n'\n",
    "              f'Total mentioned movie number (test): {n2}\\n'\n",
    "              f'Total mentioned movie number in unique (test): {len(self.Movies(False))}\\n'\n",
    "              f'Total message number (test): {m2}\\n'\n",
    "              f'Average mentioned movie numbers per conversation (train): {n1 / len1}\\n'\n",
    "              f'Average message numbers per conversation (train): {m1 / len1}\\n'\n",
    "              f'Average mentioned movie numbers per conversation (test): {n2 / len2}\\n'\n",
    "              f'Average message numbers per conversation (test): {m2 / len2}\\n\\n'\n",
    "              , end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Natural Language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYcmcjbexNwL",
    "outputId": "834a6a2c-a925-4223-f83b-8fc39844d0a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/gimdongjun/opt/anaconda3/lib/python3.9/site-packages (3.7)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/gimdongjun/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.7.9)\r\n",
      "Requirement already satisfied: tqdm in /Users/gimdongjun/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\r\n",
      "Requirement already satisfied: click in /Users/gimdongjun/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\r\n",
      "Requirement already satisfied: joblib in /Users/gimdongjun/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a library for NLP.\n",
    "- download('all'): Clean all the meaningless words (punch, stopwords, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qu2nmI4Wx8RM",
    "outputId": "03f1bb6b-8e47-4c0c-a2a3-3a32a6015670"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('all')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2SgonLUfx0ro"
   },
   "outputs": [],
   "source": [
    "from redial_parser import RedialParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Clear the special character and extract the text and movie indices\n",
    "- example: \"I like animations like @84779 and @191602\" → [I like animations like 84779 and 191602], [84779, 191602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4neDPtjJkp5U",
    "outputId": "cb1999a0-2b5d-4d71-b24a-ae07da94e602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brief information:\n",
      "Length of train data: 10006\n",
      "Length of test data: 1342\n",
      "\n",
      "Data information:\n",
      "Key parameters: ['movieMentions', 'respondentQuestions', 'messages', 'conversationId', 'respondentWorkerId', 'initiatorWorkerId', 'initiatorQuestions']\n",
      "Key parameters in Questions: ['suggested', 'seen', 'liked']\n",
      "Key parameters in messages: ['timeOffset', 'text', 'senderWorkerId', 'messageId']\n",
      "\n",
      "Context information:\n",
      "Total mentioned movie number (train): 52918\n",
      "Total mentioned movie number in unique (train): 6223\n",
      "Total message number (train): 182150\n",
      "Total mentioned movie number (test): 7154\n",
      "Total mentioned movie number in unique (test): 2007\n",
      "Total message number (test): 23952\n",
      "Average mentioned movie numbers per conversation (train): 5.288626823905656\n",
      "Average message numbers per conversation (train): 18.20407755346792\n",
      "Average mentioned movie numbers per conversation (test): 5.330849478390462\n",
      "Average message numbers per conversation (test): 17.847988077496275\n",
      "\n",
      "num: 10006\n"
     ]
    }
   ],
   "source": [
    "# Special character removal\n",
    "import re\n",
    "\n",
    "compile = re.compile(\"\\W+\")\n",
    "line3 = []  # line3: list of whole lines (global line data)\n",
    "name = []  # list of all movie names (global movie name data)\n",
    "\n",
    "parser = RedialParser('/Users/gimdongjun/MachineLearningTP/dataset')\n",
    "parser.describe()\n",
    "\n",
    "num = len(parser.train)\n",
    "print(f'num: {num}')\n",
    "\n",
    "# Note: limited the size of list to 10 because of the size error\n",
    "#maximum_num = num\n",
    "maximum_num = num\n",
    "\n",
    "movienum = [[] for _ in range(maximum_num)]  # list of movie names (local movie name data)\n",
    "line2 = [[] for _ in range(maximum_num)]  # list of lines (local line data)\n",
    "\n",
    "for i in range(maximum_num):\n",
    "    for msg in parser.train[i]['messages']:  # append line to the lists\n",
    "        line2[i].append(msg[\"text\"])\n",
    "        line3.append(msg[\"text\"])\n",
    "\n",
    "    for idx, line in enumerate(line2[i]):\n",
    "      a = compile.sub(\" \", line)  # Clear special character\n",
    "      line = line2[i][idx] = a.lower()  # lower character\n",
    "      \n",
    "      numbers = re.findall(r'\\d+', line)\n",
    "      for k in range(len(numbers)):\n",
    "        if len(numbers[k]) >= 4:\n",
    "          movienum[i].append(numbers[k])\n",
    "          name.append(numbers[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjZrBac2xNeH"
   },
   "source": [
    "Tokenization: Extract the terms related to the movies\n",
    "\n",
    "1. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library import\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R99w32o_MrGZ",
    "outputId": "aca1df01-ac2e-4ffc-88a5-273874e7820f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          id  comedy  scary  love  animation  artistic  war  sci  blood  hero  \\\n0      84779       0      0     0          1         0    0    0      0     0   \n1     191602       0      0     0          1         0    0    0      0     0   \n2     122159       0      0     0          1         0    0    0      0     0   \n3     165710       0      0     0          1         0    0    0      0     0   \n4     151313       0      0     0          1         0    0    0      0     0   \n...      ...     ...    ...   ...        ...       ...  ...  ...    ...   ...   \n5170  205981       0      0     0          1         0    0    0      0     0   \n5171  106113       0      0     0          0         0    0    1      0     0   \n5172   96852       0      0     0          0         0    0    0      0     0   \n5173  112404       0      0     0          0         0    0    0      0     0   \n5174  200018       0      0     0          0         0    0    0      0     0   \n\n      romantic  action  \n0            0       0  \n1            0       0  \n2            0       0  \n3            0       0  \n4            0       0  \n...        ...     ...  \n5170         0       0  \n5171         0       0  \n5172         0       1  \n5173         0       1  \n5174         0       1  \n\n[5175 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comedy</th>\n      <th>scary</th>\n      <th>love</th>\n      <th>animation</th>\n      <th>artistic</th>\n      <th>war</th>\n      <th>sci</th>\n      <th>blood</th>\n      <th>hero</th>\n      <th>romantic</th>\n      <th>action</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>84779</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>191602</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>122159</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165710</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>151313</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5170</th>\n      <td>205981</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5171</th>\n      <td>106113</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5172</th>\n      <td>96852</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5173</th>\n      <td>112404</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5174</th>\n      <td>200018</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5175 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word_eng = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(maximum_num):\n",
    "  line2[i] = [j for j in line2[i] if j not in stop_word_eng]  # Clear stopwords\n",
    "\n",
    "# Lemmatizer class\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "mv_tags = ['comedy','scary','love','animation','artistic','war','sci','blood','hero','romantic','action']\n",
    "x = pd.DataFrame(columns=['id'] + mv_tags)\n",
    "\n",
    "for i in range(maximum_num):\n",
    "  result_pre_lem = [token.tokenize(j) for j in line2[i]]\n",
    "  middle_pre_lem = [r for j in result_pre_lem for r in j]\n",
    "  final_lem = [lemmatizer.lemmatize(j) for j in middle_pre_lem if not j in stop_word_eng]  # Remove stopword\n",
    "\n",
    "  # Lemmatization\n",
    "  english = pd.Series(final_lem)\n",
    "  for j in english:\n",
    "    if j in mv_tags:\n",
    "      for k in movienum[i]:\n",
    "        if x[x['id'] == k].empty:\n",
    "          new_row = pd.DataFrame({'id': [k]}, columns=x.columns)\n",
    "          x = pd.concat([x, new_row], ignore_index=True)\n",
    "          x.fillna(0, inplace=True)\n",
    "          x.loc[x['id'] == k, j] += 1\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcyQgtaqxVG_"
   },
   "source": [
    "# TF-IDF\n",
    "Calculate the importance of terms in documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Construct dataframe with [movieid - all terms in dialog] relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldta9xfWOgTJ",
    "outputId": "b16f7987-c56f-4dd4-bc87-749d96162cc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      movieid                                             dialog\n0       84779  hi there how are you i m looking for movie rec...\n1      191602  hi there how are you i m looking for movie rec...\n2      122159  hi there how are you i m looking for movie rec...\n3      165710  hi there how are you i m looking for movie rec...\n4      151313  hi there how are you i m looking for movie rec...\n...       ...                                                ...\n65013  204974  what type of movies do you like hi i m looking...\n65014   85036  hello hihow can i help youso some of the movie...\n65015  170277  hello hihow can i help youso some of the movie...\n65016  149938  hello hihow can i help youso some of the movie...\n65017  200018  hello hihow can i help youso some of the movie...\n\n[65018 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieid</th>\n      <th>dialog</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>84779</td>\n      <td>hi there how are you i m looking for movie rec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>191602</td>\n      <td>hi there how are you i m looking for movie rec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>122159</td>\n      <td>hi there how are you i m looking for movie rec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165710</td>\n      <td>hi there how are you i m looking for movie rec...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>151313</td>\n      <td>hi there how are you i m looking for movie rec...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>65013</th>\n      <td>204974</td>\n      <td>what type of movies do you like hi i m looking...</td>\n    </tr>\n    <tr>\n      <th>65014</th>\n      <td>85036</td>\n      <td>hello hihow can i help youso some of the movie...</td>\n    </tr>\n    <tr>\n      <th>65015</th>\n      <td>170277</td>\n      <td>hello hihow can i help youso some of the movie...</td>\n    </tr>\n    <tr>\n      <th>65016</th>\n      <td>149938</td>\n      <td>hello hihow can i help youso some of the movie...</td>\n    </tr>\n    <tr>\n      <th>65017</th>\n      <td>200018</td>\n      <td>hello hihow can i help youso some of the movie...</td>\n    </tr>\n  </tbody>\n</table>\n<p>65018 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"movieid\", \"dialog\"])\n",
    "\n",
    "for i in range(maximum_num):\n",
    "  dig = ''\n",
    "  for line in line2[i]:  # concatenate all sentences in related message dialog\n",
    "    dig += str(line)\n",
    "  \n",
    "  for mv in movienum[i]:\n",
    "    newrow = pd.DataFrame({'movieid': [mv], 'dialog': [dig]}, columns=df.columns)\n",
    "    df = pd.concat([df, newrow], ignore_index=True)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NyIq0G5jKgIN",
    "outputId": "cd96f8a1-b3a8-48aa-e0ac-f354d66f4d05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0        hi there how are you i m looking for movie rec...\n1        hi there how are you i m looking for movie rec...\n2        hi there how are you i m looking for movie rec...\n3        hi there how are you i m looking for movie rec...\n4        hi there how are you i m looking for movie rec...\n                               ...                        \n65013    what type of movies do you like hi i m looking...\n65014    hello hihow can i help youso some of the movie...\n65015    hello hihow can i help youso some of the movie...\n65016    hello hihow can i help youso some of the movie...\n65017    hello hihow can i help youso some of the movie...\nName: dialog, Length: 65018, dtype: object"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill NaN with empty sentence\n",
    "df['dialog'].fillna('', inplace=True)\n",
    "df['dialog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make a TF-IDFs matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj2ZYa1xNvhi",
    "outputId": "28fd7f50-8411-4ccb-83b0-c583ba85a84a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           id       bye check       day       did     enjoy      good  \\\n0       84779  0.198274   0.0       0.0       0.0  0.244056  0.223147   \n1      191602  0.198274   0.0       0.0       0.0  0.244056  0.223147   \n2      122159  0.198274   0.0       0.0       0.0  0.244056  0.223147   \n3      165710  0.198274   0.0       0.0       0.0  0.244056  0.223147   \n4      151313  0.198274   0.0       0.0       0.0  0.244056  0.223147   \n...       ...       ...   ...       ...       ...       ...       ...   \n65013  204974       0.0   0.0       0.0  0.175039       0.0  0.240165   \n65014   85036       0.0   0.0  0.417921       0.0  0.156197  0.214223   \n65015  170277       0.0   0.0  0.417921       0.0  0.156197  0.214223   \n65016  149938       0.0   0.0  0.417921       0.0  0.156197  0.214223   \n65017  200018       0.0   0.0  0.417921       0.0  0.156197  0.214223   \n\n          great     haven heard  ... recommend       saw      seen  \\\n0           0.0       0.0   0.0  ...       0.0       0.0       0.0   \n1           0.0       0.0   0.0  ...       0.0       0.0       0.0   \n2           0.0       0.0   0.0  ...       0.0       0.0       0.0   \n3           0.0       0.0   0.0  ...       0.0       0.0       0.0   \n4           0.0       0.0   0.0  ...       0.0       0.0       0.0   \n...         ...       ...   ...  ...       ...       ...       ...   \n65013       0.0  0.136755   0.0  ...       0.0  0.176155  0.082112   \n65014  0.091721  0.121983   0.0  ...       0.0  0.157127  0.073242   \n65015  0.091721  0.121983   0.0  ...       0.0  0.157127  0.073242   \n65016  0.091721  0.121983   0.0  ...       0.0  0.157127  0.073242   \n65017  0.091721  0.121983   0.0  ...       0.0  0.157127  0.073242   \n\n      suggestions     thank    thanks     think   ve     watch  yes  \n0             0.0       0.0  0.183523       0.0  0.0       0.0  0.0  \n1             0.0       0.0  0.183523       0.0  0.0       0.0  0.0  \n2             0.0       0.0  0.183523       0.0  0.0       0.0  0.0  \n3             0.0       0.0  0.183523       0.0  0.0       0.0  0.0  \n4             0.0       0.0  0.183523       0.0  0.0       0.0  0.0  \n...           ...       ...       ...       ...  ...       ...  ...  \n65013    0.307302       0.0  0.131679       0.0  0.0  0.136393  0.0  \n65014         0.0  0.407384       0.0  0.376058  0.0  0.364981  0.0  \n65015         0.0  0.407384       0.0  0.376058  0.0  0.364981  0.0  \n65016         0.0  0.407384       0.0  0.376058  0.0  0.364981  0.0  \n65017         0.0  0.407384       0.0  0.376058  0.0  0.364981  0.0  \n\n[65018 rows x 35 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>bye</th>\n      <th>check</th>\n      <th>day</th>\n      <th>did</th>\n      <th>enjoy</th>\n      <th>good</th>\n      <th>great</th>\n      <th>haven</th>\n      <th>heard</th>\n      <th>...</th>\n      <th>recommend</th>\n      <th>saw</th>\n      <th>seen</th>\n      <th>suggestions</th>\n      <th>thank</th>\n      <th>thanks</th>\n      <th>think</th>\n      <th>ve</th>\n      <th>watch</th>\n      <th>yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>84779</td>\n      <td>0.198274</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.244056</td>\n      <td>0.223147</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.183523</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>191602</td>\n      <td>0.198274</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.244056</td>\n      <td>0.223147</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.183523</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>122159</td>\n      <td>0.198274</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.244056</td>\n      <td>0.223147</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.183523</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165710</td>\n      <td>0.198274</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.244056</td>\n      <td>0.223147</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.183523</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>151313</td>\n      <td>0.198274</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.244056</td>\n      <td>0.223147</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.183523</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>65013</th>\n      <td>204974</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.175039</td>\n      <td>0.0</td>\n      <td>0.240165</td>\n      <td>0.0</td>\n      <td>0.136755</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.176155</td>\n      <td>0.082112</td>\n      <td>0.307302</td>\n      <td>0.0</td>\n      <td>0.131679</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.136393</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>65014</th>\n      <td>85036</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.417921</td>\n      <td>0.0</td>\n      <td>0.156197</td>\n      <td>0.214223</td>\n      <td>0.091721</td>\n      <td>0.121983</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.157127</td>\n      <td>0.073242</td>\n      <td>0.0</td>\n      <td>0.407384</td>\n      <td>0.0</td>\n      <td>0.376058</td>\n      <td>0.0</td>\n      <td>0.364981</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>65015</th>\n      <td>170277</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.417921</td>\n      <td>0.0</td>\n      <td>0.156197</td>\n      <td>0.214223</td>\n      <td>0.091721</td>\n      <td>0.121983</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.157127</td>\n      <td>0.073242</td>\n      <td>0.0</td>\n      <td>0.407384</td>\n      <td>0.0</td>\n      <td>0.376058</td>\n      <td>0.0</td>\n      <td>0.364981</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>65016</th>\n      <td>149938</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.417921</td>\n      <td>0.0</td>\n      <td>0.156197</td>\n      <td>0.214223</td>\n      <td>0.091721</td>\n      <td>0.121983</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.157127</td>\n      <td>0.073242</td>\n      <td>0.0</td>\n      <td>0.407384</td>\n      <td>0.0</td>\n      <td>0.376058</td>\n      <td>0.0</td>\n      <td>0.364981</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>65017</th>\n      <td>200018</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.417921</td>\n      <td>0.0</td>\n      <td>0.156197</td>\n      <td>0.214223</td>\n      <td>0.091721</td>\n      <td>0.121983</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.157127</td>\n      <td>0.073242</td>\n      <td>0.0</td>\n      <td>0.407384</td>\n      <td>0.0</td>\n      <td>0.376058</td>\n      <td>0.0</td>\n      <td>0.364981</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>65018 rows × 35 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizer class\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=0.2)  # Ignore English Stopwords\n",
    "\n",
    "# Obtain matrix\n",
    "tfidf_mat = tfidf.fit_transform(df['dialog']).toarray()\n",
    "\n",
    "# Construct dataset with id + word vectors\n",
    "cdata = np.concatenate((df['movieid'].to_numpy().reshape(len(df['dialog']), 1), tfidf_mat), axis=1)\n",
    "df_mv_tfidf = pd.DataFrame(cdata, columns=['id'] + tfidf.get_feature_names_out().tolist())\n",
    "df_mv_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'bye', 'check', 'day', 'did', 'enjoy', 'good', 'great', 'haven',\n",
      "       'heard', 'hello', 'help', 'hi', 'hope', 'kind', 'know', 'like', 'liked',\n",
      "       'll', 'looking', 'love', 'loved', 'movie', 'movies', 'really',\n",
      "       'recommend', 'saw', 'seen', 'suggestions', 'thank', 'thanks', 'think',\n",
      "       've', 'watch', 'yes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_mv_tfidf.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "id                84779\nbye            0.198274\ncheck               0.0\nday                 0.0\ndid                 0.0\nenjoy          0.244056\ngood           0.223147\ngreat               0.0\nhaven               0.0\nheard               0.0\nhello               0.0\nhelp                0.0\nhi              0.18647\nhope                0.0\nkind            0.21227\nknow                0.0\nlike            0.78617\nliked               0.0\nll                  0.0\nlooking         0.17163\nlove                0.0\nloved               0.0\nmovie           0.27069\nmovies         0.130485\nreally              0.0\nrecommend           0.0\nsaw                 0.0\nseen                0.0\nsuggestions         0.0\nthank               0.0\nthanks         0.183523\nthink               0.0\nve                  0.0\nwatch               0.0\nyes                 0.0\nName: 0, dtype: object"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mv_tfidf.iloc[0,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "              bye     check       day       did     enjoy      good     great  \\\nid                                                                              \n0104807  0.166044  0.000000  0.000000  0.408596  0.000000  0.280311  0.000000   \n1000     0.024181  0.212646  0.079637  0.029752  0.000000  0.160218  0.093477   \n100026   0.151635  0.149872  0.000000  0.186570  0.000000  0.426643  0.109602   \n100030   0.055407  0.057767  0.099412  0.066266  0.047732  0.313640  0.139210   \n100043   0.135243  0.000000  0.108377  0.144812  0.054967  0.387337  0.220982   \n...           ...       ...       ...       ...       ...       ...       ...   \n99910    0.068484  0.077972  0.078895  0.066677  0.028353  0.233398  0.120542   \n99955    0.174969  0.180397  0.192081  0.000000  0.000000  0.205415  0.000000   \n99966    0.103250  0.000000  0.000000  0.000000  0.000000  0.228472  0.218835   \n99975    0.295107  0.145838  0.000000  0.000000  0.000000  0.415159  0.106651   \n99998    0.145908  0.000000  0.160177  0.179523  0.000000  0.082106  0.210923   \n\n            haven     heard     hello  ...  recommend       saw      seen  \\\nid                                     ...                                  \n0104807  0.000000  0.195814  0.000000  ...   0.000000  0.205600  0.000000   \n1000     0.119078  0.123998  0.125702  ...   0.028736  0.029941  0.306244   \n100026   0.291527  0.178821  0.153871  ...   0.000000  0.375517  0.175042   \n100030   0.095911  0.064867  0.058756  ...   0.068676  0.036386  0.183518   \n100043   0.051973  0.105325  0.082373  ...   0.053068  0.122241  0.235002   \n...           ...       ...       ...  ...        ...       ...       ...   \n99910    0.083755  0.059995  0.105850  ...   0.062958  0.046926  0.224293   \n99955    0.000000  0.000000  0.000000  ...   0.216902  0.000000  0.206335   \n99966    0.097012  0.000000  0.000000  ...   0.000000  0.000000  0.116499   \n99975    0.283680  0.348016  0.000000  ...   0.175350  0.000000  0.170330   \n99998    0.000000  0.344134  0.000000  ...   0.000000  0.000000  0.000000   \n\n         suggestions     thank    thanks     think        ve     watch  \\\nid                                                                       \n0104807     0.179335  0.000000  0.461071  0.328047  0.000000  0.000000   \n1000        0.000000  0.169466  0.000000  0.090161  0.091957  0.115915   \n100026      0.163773  0.162268  0.000000  0.000000  0.000000  0.290756   \n100030      0.100501  0.057128  0.074413  0.057178  0.022839  0.096594   \n100043      0.000000  0.057858  0.083847  0.080188  0.041478  0.051835   \n...              ...       ...       ...       ...       ...       ...   \n99910       0.071120  0.047272  0.093565  0.106648  0.084666  0.075178   \n99955       0.000000  0.000000  0.161952  0.000000  0.000000  0.000000   \n99966       0.111514  0.110489  0.000000  0.000000  0.000000  0.096756   \n99975       0.000000  0.000000  0.136575  0.145758  0.000000  0.000000   \n99998       0.000000  0.312277  0.270104  0.000000  0.000000  0.559546   \n\n              yes  \nid                 \n0104807  0.170997  \n1000     0.241468  \n100026   0.000000  \n100030   0.071554  \n100043   0.093289  \n...           ...  \n99910    0.082360  \n99955    0.000000  \n99966    0.103931  \n99975    0.000000  \n99998    0.150260  \n\n[6311 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bye</th>\n      <th>check</th>\n      <th>day</th>\n      <th>did</th>\n      <th>enjoy</th>\n      <th>good</th>\n      <th>great</th>\n      <th>haven</th>\n      <th>heard</th>\n      <th>hello</th>\n      <th>...</th>\n      <th>recommend</th>\n      <th>saw</th>\n      <th>seen</th>\n      <th>suggestions</th>\n      <th>thank</th>\n      <th>thanks</th>\n      <th>think</th>\n      <th>ve</th>\n      <th>watch</th>\n      <th>yes</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0104807</th>\n      <td>0.166044</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.408596</td>\n      <td>0.000000</td>\n      <td>0.280311</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.195814</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.205600</td>\n      <td>0.000000</td>\n      <td>0.179335</td>\n      <td>0.000000</td>\n      <td>0.461071</td>\n      <td>0.328047</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.170997</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>0.024181</td>\n      <td>0.212646</td>\n      <td>0.079637</td>\n      <td>0.029752</td>\n      <td>0.000000</td>\n      <td>0.160218</td>\n      <td>0.093477</td>\n      <td>0.119078</td>\n      <td>0.123998</td>\n      <td>0.125702</td>\n      <td>...</td>\n      <td>0.028736</td>\n      <td>0.029941</td>\n      <td>0.306244</td>\n      <td>0.000000</td>\n      <td>0.169466</td>\n      <td>0.000000</td>\n      <td>0.090161</td>\n      <td>0.091957</td>\n      <td>0.115915</td>\n      <td>0.241468</td>\n    </tr>\n    <tr>\n      <th>100026</th>\n      <td>0.151635</td>\n      <td>0.149872</td>\n      <td>0.000000</td>\n      <td>0.186570</td>\n      <td>0.000000</td>\n      <td>0.426643</td>\n      <td>0.109602</td>\n      <td>0.291527</td>\n      <td>0.178821</td>\n      <td>0.153871</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.375517</td>\n      <td>0.175042</td>\n      <td>0.163773</td>\n      <td>0.162268</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.290756</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>100030</th>\n      <td>0.055407</td>\n      <td>0.057767</td>\n      <td>0.099412</td>\n      <td>0.066266</td>\n      <td>0.047732</td>\n      <td>0.313640</td>\n      <td>0.139210</td>\n      <td>0.095911</td>\n      <td>0.064867</td>\n      <td>0.058756</td>\n      <td>...</td>\n      <td>0.068676</td>\n      <td>0.036386</td>\n      <td>0.183518</td>\n      <td>0.100501</td>\n      <td>0.057128</td>\n      <td>0.074413</td>\n      <td>0.057178</td>\n      <td>0.022839</td>\n      <td>0.096594</td>\n      <td>0.071554</td>\n    </tr>\n    <tr>\n      <th>100043</th>\n      <td>0.135243</td>\n      <td>0.000000</td>\n      <td>0.108377</td>\n      <td>0.144812</td>\n      <td>0.054967</td>\n      <td>0.387337</td>\n      <td>0.220982</td>\n      <td>0.051973</td>\n      <td>0.105325</td>\n      <td>0.082373</td>\n      <td>...</td>\n      <td>0.053068</td>\n      <td>0.122241</td>\n      <td>0.235002</td>\n      <td>0.000000</td>\n      <td>0.057858</td>\n      <td>0.083847</td>\n      <td>0.080188</td>\n      <td>0.041478</td>\n      <td>0.051835</td>\n      <td>0.093289</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99910</th>\n      <td>0.068484</td>\n      <td>0.077972</td>\n      <td>0.078895</td>\n      <td>0.066677</td>\n      <td>0.028353</td>\n      <td>0.233398</td>\n      <td>0.120542</td>\n      <td>0.083755</td>\n      <td>0.059995</td>\n      <td>0.105850</td>\n      <td>...</td>\n      <td>0.062958</td>\n      <td>0.046926</td>\n      <td>0.224293</td>\n      <td>0.071120</td>\n      <td>0.047272</td>\n      <td>0.093565</td>\n      <td>0.106648</td>\n      <td>0.084666</td>\n      <td>0.075178</td>\n      <td>0.082360</td>\n    </tr>\n    <tr>\n      <th>99955</th>\n      <td>0.174969</td>\n      <td>0.180397</td>\n      <td>0.192081</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.205415</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.216902</td>\n      <td>0.000000</td>\n      <td>0.206335</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.161952</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>99966</th>\n      <td>0.103250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.228472</td>\n      <td>0.218835</td>\n      <td>0.097012</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.116499</td>\n      <td>0.111514</td>\n      <td>0.110489</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.096756</td>\n      <td>0.103931</td>\n    </tr>\n    <tr>\n      <th>99975</th>\n      <td>0.295107</td>\n      <td>0.145838</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.415159</td>\n      <td>0.106651</td>\n      <td>0.283680</td>\n      <td>0.348016</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.175350</td>\n      <td>0.000000</td>\n      <td>0.170330</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.136575</td>\n      <td>0.145758</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>0.145908</td>\n      <td>0.000000</td>\n      <td>0.160177</td>\n      <td>0.179523</td>\n      <td>0.000000</td>\n      <td>0.082106</td>\n      <td>0.210923</td>\n      <td>0.000000</td>\n      <td>0.344134</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.312277</td>\n      <td>0.270104</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.559546</td>\n      <td>0.150260</td>\n    </tr>\n  </tbody>\n</table>\n<p>6311 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 프레임을 변경해야 될 것 같음, 지금 데이터 프레임을 transpose하면 안된다\n",
    "# https://techblog-history-younghunjo1.tistory.com/116\n",
    "# 위 블로그를 참고해서 변경하자\n",
    "# 우선 영화별로 tf-id 그룹핑\n",
    "df_grouped = df_mv_tfidf.groupby('id').mean()\n",
    "df_grouped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "row가 6311개로 감소한 것을 확인 할 수 있습니다. 각 컬럼 값은 각 평균 값으로 합쳐졌습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "favor_col = ['enjoy', 'good', 'great','like', 'liked','love', 'loved','recommend','suggestions']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "id        0104807      1000    100026    100030    100043    100070    100074  \\\nid                                                                              \n0104807  1.000000  0.490322  0.505161  0.588107  0.559630  0.597002  0.245053   \n1000     0.490322  1.000000  0.514527  0.757324  0.604548  0.805039  0.486424   \n100026   0.505161  0.514527  1.000000  0.706399  0.703771  0.627103  0.422338   \n100030   0.588107  0.757324  0.706399  1.000000  0.875844  0.881791  0.589227   \n100043   0.559630  0.604548  0.703771  0.875844  1.000000  0.704115  0.452371   \n...           ...       ...       ...       ...       ...       ...       ...   \n99910    0.644824  0.876839  0.657381  0.946464  0.825630  0.913149  0.577033   \n99955    0.375656  0.647498  0.377880  0.665634  0.547735  0.682649  0.445091   \n99966    0.247013  0.455872  0.419437  0.713804  0.625198  0.542590  0.277880   \n99975    0.549524  0.604381  0.647451  0.773232  0.695827  0.705064  0.437532   \n99998    0.520652  0.545038  0.527934  0.558631  0.479755  0.640536  0.351602   \n\nid         100106    100165    100178  ...     99809     99812     99824  \\\nid                                     ...                                 \n0104807  0.565411  0.234531  0.531345  ...  0.551106  0.195945  0.414124   \n1000     0.604776  0.440788  0.631587  ...  0.713208  0.579117  0.498381   \n100026   0.704274  0.396074  0.458058  ...  0.639047  0.455666  0.355907   \n100030   0.699875  0.541173  0.772845  ...  0.820708  0.597180  0.687088   \n100043   0.675797  0.579099  0.683312  ...  0.733105  0.515025  0.519787   \n...           ...       ...       ...  ...       ...       ...       ...   \n99910    0.723025  0.538627  0.761116  ...  0.839207  0.586364  0.677236   \n99955    0.566404  0.382931  0.580718  ...  0.670102  0.437906  0.533221   \n99966    0.471678  0.374122  0.802528  ...  0.492426  0.294270  0.639714   \n99975    0.729970  0.377786  0.610428  ...  0.680450  0.494739  0.522480   \n99998    0.591345  0.325210  0.477063  ...  0.598892  0.521082  0.268753   \n\nid          99887     99896     99910     99955     99966     99975     99998  \nid                                                                             \n0104807  0.416001  0.643700  0.644824  0.375656  0.247013  0.549524  0.520652  \n1000     0.599368  0.840864  0.876839  0.647498  0.455872  0.604381  0.545038  \n100026   0.680402  0.662055  0.657381  0.377880  0.419437  0.647451  0.527934  \n100030   0.773864  0.918302  0.946464  0.665634  0.713804  0.773232  0.558631  \n100043   0.684666  0.804357  0.825630  0.547735  0.625198  0.695827  0.479755  \n...           ...       ...       ...       ...       ...       ...       ...  \n99910    0.724868  0.962434  1.000000  0.709384  0.628625  0.764047  0.558634  \n99955    0.535804  0.733398  0.709384  1.000000  0.415360  0.654425  0.345944  \n99966    0.735600  0.660884  0.628625  0.415360  1.000000  0.593664  0.299888  \n99975    0.634413  0.757762  0.764047  0.654425  0.593664  1.000000  0.383732  \n99998    0.302161  0.566909  0.558634  0.345944  0.299888  0.383732  1.000000  \n\n[6311 rows x 6311 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>id</th>\n      <th>0104807</th>\n      <th>1000</th>\n      <th>100026</th>\n      <th>100030</th>\n      <th>100043</th>\n      <th>100070</th>\n      <th>100074</th>\n      <th>100106</th>\n      <th>100165</th>\n      <th>100178</th>\n      <th>...</th>\n      <th>99809</th>\n      <th>99812</th>\n      <th>99824</th>\n      <th>99887</th>\n      <th>99896</th>\n      <th>99910</th>\n      <th>99955</th>\n      <th>99966</th>\n      <th>99975</th>\n      <th>99998</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0104807</th>\n      <td>1.000000</td>\n      <td>0.490322</td>\n      <td>0.505161</td>\n      <td>0.588107</td>\n      <td>0.559630</td>\n      <td>0.597002</td>\n      <td>0.245053</td>\n      <td>0.565411</td>\n      <td>0.234531</td>\n      <td>0.531345</td>\n      <td>...</td>\n      <td>0.551106</td>\n      <td>0.195945</td>\n      <td>0.414124</td>\n      <td>0.416001</td>\n      <td>0.643700</td>\n      <td>0.644824</td>\n      <td>0.375656</td>\n      <td>0.247013</td>\n      <td>0.549524</td>\n      <td>0.520652</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>0.490322</td>\n      <td>1.000000</td>\n      <td>0.514527</td>\n      <td>0.757324</td>\n      <td>0.604548</td>\n      <td>0.805039</td>\n      <td>0.486424</td>\n      <td>0.604776</td>\n      <td>0.440788</td>\n      <td>0.631587</td>\n      <td>...</td>\n      <td>0.713208</td>\n      <td>0.579117</td>\n      <td>0.498381</td>\n      <td>0.599368</td>\n      <td>0.840864</td>\n      <td>0.876839</td>\n      <td>0.647498</td>\n      <td>0.455872</td>\n      <td>0.604381</td>\n      <td>0.545038</td>\n    </tr>\n    <tr>\n      <th>100026</th>\n      <td>0.505161</td>\n      <td>0.514527</td>\n      <td>1.000000</td>\n      <td>0.706399</td>\n      <td>0.703771</td>\n      <td>0.627103</td>\n      <td>0.422338</td>\n      <td>0.704274</td>\n      <td>0.396074</td>\n      <td>0.458058</td>\n      <td>...</td>\n      <td>0.639047</td>\n      <td>0.455666</td>\n      <td>0.355907</td>\n      <td>0.680402</td>\n      <td>0.662055</td>\n      <td>0.657381</td>\n      <td>0.377880</td>\n      <td>0.419437</td>\n      <td>0.647451</td>\n      <td>0.527934</td>\n    </tr>\n    <tr>\n      <th>100030</th>\n      <td>0.588107</td>\n      <td>0.757324</td>\n      <td>0.706399</td>\n      <td>1.000000</td>\n      <td>0.875844</td>\n      <td>0.881791</td>\n      <td>0.589227</td>\n      <td>0.699875</td>\n      <td>0.541173</td>\n      <td>0.772845</td>\n      <td>...</td>\n      <td>0.820708</td>\n      <td>0.597180</td>\n      <td>0.687088</td>\n      <td>0.773864</td>\n      <td>0.918302</td>\n      <td>0.946464</td>\n      <td>0.665634</td>\n      <td>0.713804</td>\n      <td>0.773232</td>\n      <td>0.558631</td>\n    </tr>\n    <tr>\n      <th>100043</th>\n      <td>0.559630</td>\n      <td>0.604548</td>\n      <td>0.703771</td>\n      <td>0.875844</td>\n      <td>1.000000</td>\n      <td>0.704115</td>\n      <td>0.452371</td>\n      <td>0.675797</td>\n      <td>0.579099</td>\n      <td>0.683312</td>\n      <td>...</td>\n      <td>0.733105</td>\n      <td>0.515025</td>\n      <td>0.519787</td>\n      <td>0.684666</td>\n      <td>0.804357</td>\n      <td>0.825630</td>\n      <td>0.547735</td>\n      <td>0.625198</td>\n      <td>0.695827</td>\n      <td>0.479755</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99910</th>\n      <td>0.644824</td>\n      <td>0.876839</td>\n      <td>0.657381</td>\n      <td>0.946464</td>\n      <td>0.825630</td>\n      <td>0.913149</td>\n      <td>0.577033</td>\n      <td>0.723025</td>\n      <td>0.538627</td>\n      <td>0.761116</td>\n      <td>...</td>\n      <td>0.839207</td>\n      <td>0.586364</td>\n      <td>0.677236</td>\n      <td>0.724868</td>\n      <td>0.962434</td>\n      <td>1.000000</td>\n      <td>0.709384</td>\n      <td>0.628625</td>\n      <td>0.764047</td>\n      <td>0.558634</td>\n    </tr>\n    <tr>\n      <th>99955</th>\n      <td>0.375656</td>\n      <td>0.647498</td>\n      <td>0.377880</td>\n      <td>0.665634</td>\n      <td>0.547735</td>\n      <td>0.682649</td>\n      <td>0.445091</td>\n      <td>0.566404</td>\n      <td>0.382931</td>\n      <td>0.580718</td>\n      <td>...</td>\n      <td>0.670102</td>\n      <td>0.437906</td>\n      <td>0.533221</td>\n      <td>0.535804</td>\n      <td>0.733398</td>\n      <td>0.709384</td>\n      <td>1.000000</td>\n      <td>0.415360</td>\n      <td>0.654425</td>\n      <td>0.345944</td>\n    </tr>\n    <tr>\n      <th>99966</th>\n      <td>0.247013</td>\n      <td>0.455872</td>\n      <td>0.419437</td>\n      <td>0.713804</td>\n      <td>0.625198</td>\n      <td>0.542590</td>\n      <td>0.277880</td>\n      <td>0.471678</td>\n      <td>0.374122</td>\n      <td>0.802528</td>\n      <td>...</td>\n      <td>0.492426</td>\n      <td>0.294270</td>\n      <td>0.639714</td>\n      <td>0.735600</td>\n      <td>0.660884</td>\n      <td>0.628625</td>\n      <td>0.415360</td>\n      <td>1.000000</td>\n      <td>0.593664</td>\n      <td>0.299888</td>\n    </tr>\n    <tr>\n      <th>99975</th>\n      <td>0.549524</td>\n      <td>0.604381</td>\n      <td>0.647451</td>\n      <td>0.773232</td>\n      <td>0.695827</td>\n      <td>0.705064</td>\n      <td>0.437532</td>\n      <td>0.729970</td>\n      <td>0.377786</td>\n      <td>0.610428</td>\n      <td>...</td>\n      <td>0.680450</td>\n      <td>0.494739</td>\n      <td>0.522480</td>\n      <td>0.634413</td>\n      <td>0.757762</td>\n      <td>0.764047</td>\n      <td>0.654425</td>\n      <td>0.593664</td>\n      <td>1.000000</td>\n      <td>0.383732</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>0.520652</td>\n      <td>0.545038</td>\n      <td>0.527934</td>\n      <td>0.558631</td>\n      <td>0.479755</td>\n      <td>0.640536</td>\n      <td>0.351602</td>\n      <td>0.591345</td>\n      <td>0.325210</td>\n      <td>0.477063</td>\n      <td>...</td>\n      <td>0.598892</td>\n      <td>0.521082</td>\n      <td>0.268753</td>\n      <td>0.302161</td>\n      <td>0.566909</td>\n      <td>0.558634</td>\n      <td>0.345944</td>\n      <td>0.299888</td>\n      <td>0.383732</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>6311 rows × 6311 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "word_sim = cosine_similarity(df_grouped, df_grouped)\n",
    "df_word_sim = pd.DataFrame(word_sim, index = df_grouped.index, columns=df_grouped.index)\n",
    "df_word_sim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "영화들간의 코사인 유사도를 계산하였습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "id            0104807      1000    100026    100030    100043    100070  \\\nbye          0.083689  0.079910  0.082787  0.080793  0.082266  0.081221   \ncheck        0.084713  0.090644  0.088666  0.086869  0.085038  0.087435   \nday          0.063981  0.066039  0.064580  0.067171  0.067424  0.066501   \ndid          0.069010  0.059463  0.062672  0.060097  0.061466  0.060028   \nenjoy        0.056569  0.056390  0.056095  0.057481  0.057828  0.057457   \ngood         0.241609  0.234526  0.244435  0.240639  0.242084  0.237324   \ngreat        0.137276  0.140868  0.141685  0.143314  0.145994  0.142050   \nhaven        0.085307  0.091002  0.093368  0.089080  0.087781  0.087026   \nheard        0.066156  0.064463  0.066382  0.063570  0.064001  0.063581   \nhello        0.071780  0.074870  0.075675  0.073843  0.074234  0.074771   \nhelp         0.075927  0.076620  0.079915  0.077049  0.076108  0.080807   \nhi           0.085625  0.082834  0.085017  0.083808  0.084028  0.083372   \nhope         0.055973  0.056451  0.055987  0.057507  0.057824  0.056510   \nkind         0.066809  0.068619  0.066388  0.068187  0.068257  0.067744   \nknow         0.060548  0.059673  0.059669  0.061518  0.059713  0.062499   \nlike         0.260856  0.261079  0.242690  0.251123  0.245456  0.256354   \nliked        0.088171  0.083249  0.090089  0.085610  0.085532  0.085921   \nll           0.070876  0.075324  0.071120  0.071456  0.069562  0.071502   \nlooking      0.093841  0.096813  0.096148  0.097325  0.097315  0.096554   \nlove         0.093400  0.094157  0.093276  0.100696  0.099355  0.097598   \nloved        0.059627  0.059962  0.060581  0.062544  0.066547  0.060754   \nmovie        0.170533  0.168060  0.171154  0.168641  0.172231  0.168947   \nmovies       0.157122  0.160080  0.159727  0.160169  0.160762  0.161148   \nreally       0.110483  0.109287  0.107712  0.109011  0.109164  0.111485   \nrecommend    0.063207  0.064479  0.064442  0.065154  0.064895  0.064225   \nsaw          0.061073  0.054875  0.063082  0.055555  0.057539  0.054694   \nseen         0.200570  0.209690  0.205850  0.206133  0.206540  0.204107   \nsuggestions  0.069231  0.065972  0.069209  0.067995  0.066600  0.066469   \nthank        0.068285  0.073163  0.074600  0.071646  0.072001  0.071438   \nthanks       0.092376  0.084424  0.084489  0.086051  0.085936  0.086323   \nthink        0.099975  0.093525  0.091946  0.093073  0.093730  0.094201   \nve           0.073811  0.076842  0.073832  0.074381  0.074623  0.074070   \nwatch        0.091920  0.094397  0.099178  0.094663  0.093892  0.096185   \nyes          0.081464  0.081432  0.078356  0.079103  0.080106  0.077656   \n\nid             100074    100106    100165    100178  ...     99809     99812  \\\nbye          0.079191  0.088504  0.079324  0.082861  ...  0.083496  0.082364   \ncheck        0.091634  0.088010  0.085868  0.089288  ...  0.086496  0.091117   \nday          0.073608  0.065190  0.067399  0.068081  ...  0.065800  0.069392   \ndid          0.057763  0.061228  0.057161  0.061849  ...  0.059801  0.058177   \nenjoy        0.065243  0.057168  0.057072  0.056873  ...  0.059388  0.061095   \ngood         0.231583  0.239703  0.230516  0.232578  ...  0.237050  0.228670   \ngreat        0.153280  0.144915  0.162648  0.146934  ...  0.141895  0.149373   \nhaven        0.096032  0.090301  0.092396  0.086839  ...  0.086367  0.098713   \nheard        0.062324  0.067102  0.061790  0.062975  ...  0.063508  0.066844   \nhello        0.076971  0.073423  0.073849  0.073864  ...  0.073567  0.074179   \nhelp         0.080806  0.078995  0.076426  0.076599  ...  0.078180  0.075759   \nhi           0.083167  0.082933  0.083457  0.084266  ...  0.084860  0.083414   \nhope         0.058158  0.057885  0.056762  0.057011  ...  0.058078  0.057017   \nkind         0.066656  0.066348  0.065837  0.068399  ...  0.067739  0.066313   \nknow         0.057967  0.059087  0.058178  0.059928  ...  0.063120  0.063268   \nlike         0.243933  0.247775  0.241631  0.251013  ...  0.251024  0.244608   \nliked        0.087647  0.086110  0.082870  0.082067  ...  0.086015  0.087901   \nll           0.072199  0.071057  0.069853  0.073817  ...  0.071224  0.072359   \nlooking      0.101929  0.094250  0.096110  0.097738  ...  0.097510  0.102726   \nlove         0.093881  0.093866  0.095449  0.109037  ...  0.095294  0.093763   \nloved        0.061845  0.060751  0.062194  0.062684  ...  0.061003  0.066279   \nmovie        0.161839  0.161414  0.178253  0.166166  ...  0.170020  0.161262   \nmovies       0.158631  0.161542  0.158630  0.164020  ...  0.161353  0.155150   \nreally       0.119317  0.111048  0.107216  0.106378  ...  0.108240  0.110971   \nrecommend    0.062759  0.066904  0.074264  0.063793  ...  0.067208  0.062101   \nsaw          0.054083  0.057907  0.053621  0.055955  ...  0.056193  0.054427   \nseen         0.209607  0.208152  0.215208  0.203961  ...  0.204343  0.216950   \nsuggestions  0.070206  0.069213  0.069560  0.067299  ...  0.066982  0.067371   \nthank        0.070871  0.074389  0.071307  0.071583  ...  0.074676  0.071430   \nthanks       0.088481  0.087447  0.087876  0.087708  ...  0.084559  0.085171   \nthink        0.091521  0.091468  0.091086  0.094247  ...  0.096407  0.091432   \nve           0.074514  0.077606  0.075160  0.075289  ...  0.077670  0.076730   \nwatch        0.095627  0.096417  0.093303  0.092824  ...  0.095504  0.099886   \nyes          0.076103  0.079980  0.081831  0.079966  ...  0.078912  0.081202   \n\nid              99824     99887     99896     99910     99955     99966  \\\nbye          0.084090  0.080324  0.081488  0.081148  0.083424  0.083187   \ncheck        0.086218  0.086828  0.086211  0.087545  0.091965  0.083624   \nday          0.064942  0.064707  0.066478  0.066588  0.067914  0.066502   \ndid          0.058085  0.059492  0.060502  0.060208  0.057867  0.058221   \nenjoy        0.063313  0.056340  0.057703  0.057127  0.056057  0.056650   \ngood         0.239231  0.244478  0.236255  0.237589  0.238119  0.236497   \ngreat        0.138873  0.138090  0.143143  0.142364  0.138991  0.146593   \nhaven        0.087703  0.088089  0.088367  0.089353  0.087935  0.086964   \nheard        0.062077  0.062604  0.062917  0.063392  0.061972  0.061388   \nhello        0.074357  0.072527  0.073683  0.074530  0.073120  0.072764   \nhelp         0.080434  0.078104  0.076531  0.077398  0.075335  0.075339   \nhi           0.082425  0.082176  0.083979  0.083896  0.082829  0.083708   \nhope         0.057310  0.057959  0.057534  0.056932  0.056154  0.056294   \nkind         0.066877  0.067620  0.068748  0.068756  0.068062  0.070078   \nknow         0.059007  0.059899  0.061207  0.060675  0.064017  0.059339   \nlike         0.252169  0.251745  0.256100  0.255720  0.254759  0.252351   \nliked        0.081906  0.084862  0.085124  0.084559  0.082956  0.080826   \nll           0.073549  0.070375  0.071758  0.072389  0.076962  0.068414   \nlooking      0.093726  0.095825  0.095370  0.096610  0.097673  0.093820   \nlove         0.108415  0.104397  0.097083  0.097327  0.094830  0.117418   \nloved        0.060803  0.060861  0.061280  0.061712  0.059867  0.064877   \nmovie        0.161461  0.169255  0.167216  0.167520  0.165830  0.165193   \nmovies       0.162184  0.167592  0.163768  0.160730  0.167594  0.171501   \nreally       0.111381  0.106636  0.109329  0.108761  0.110760  0.105557   \nrecommend    0.066804  0.064262  0.065716  0.065085  0.070511  0.064351   \nsaw          0.053799  0.060170  0.056693  0.055600  0.054224  0.053604   \nseen         0.208121  0.204338  0.206446  0.207858  0.206187  0.203577   \nsuggestions  0.069593  0.067817  0.067702  0.067316  0.065754  0.068595   \nthank        0.071473  0.070750  0.071175  0.071182  0.069530  0.073208   \nthanks       0.089806  0.085006  0.086347  0.086451  0.088838  0.084340   \nthink        0.091783  0.091643  0.093908  0.094304  0.091256  0.090837   \nve           0.077335  0.075389  0.075228  0.076322  0.074535  0.072069   \nwatch        0.092034  0.094861  0.095182  0.094059  0.091990  0.096035   \nyes          0.077537  0.082192  0.080084  0.079098  0.076338  0.080115   \n\nid              99975     99998  \nbye          0.084586  0.083220  \ncheck        0.088935  0.083730  \nday          0.064624  0.068875  \ndid          0.058736  0.062416  \nenjoy        0.056140  0.056999  \ngood         0.241506  0.232954  \ngreat        0.140049  0.147788  \nhaven        0.092091  0.084955  \nheard        0.067995  0.069783  \nhello        0.072745  0.073983  \nhelp         0.075437  0.081395  \nhi           0.084524  0.086210  \nhope         0.055793  0.056382  \nkind         0.069758  0.066611  \nknow         0.059102  0.064140  \nlike         0.252592  0.254271  \nliked        0.088146  0.081311  \nll           0.071731  0.069506  \nlooking      0.096947  0.096750  \nlove         0.097292  0.094678  \nloved        0.060250  0.060934  \nmovie        0.164022  0.175361  \nmovies       0.165449  0.157358  \nreally       0.109542  0.104800  \nrecommend    0.067786  0.064807  \nsaw          0.055222  0.054888  \nseen         0.205796  0.197589  \nsuggestions  0.066765  0.066070  \nthank        0.070461  0.077006  \nthanks       0.086949  0.087465  \nthink        0.093875  0.091520  \nve           0.074300  0.073278  \nwatch        0.091927  0.108775  \nyes          0.076961  0.080652  \n\n[34 rows x 6311 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>id</th>\n      <th>0104807</th>\n      <th>1000</th>\n      <th>100026</th>\n      <th>100030</th>\n      <th>100043</th>\n      <th>100070</th>\n      <th>100074</th>\n      <th>100106</th>\n      <th>100165</th>\n      <th>100178</th>\n      <th>...</th>\n      <th>99809</th>\n      <th>99812</th>\n      <th>99824</th>\n      <th>99887</th>\n      <th>99896</th>\n      <th>99910</th>\n      <th>99955</th>\n      <th>99966</th>\n      <th>99975</th>\n      <th>99998</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bye</th>\n      <td>0.083689</td>\n      <td>0.079910</td>\n      <td>0.082787</td>\n      <td>0.080793</td>\n      <td>0.082266</td>\n      <td>0.081221</td>\n      <td>0.079191</td>\n      <td>0.088504</td>\n      <td>0.079324</td>\n      <td>0.082861</td>\n      <td>...</td>\n      <td>0.083496</td>\n      <td>0.082364</td>\n      <td>0.084090</td>\n      <td>0.080324</td>\n      <td>0.081488</td>\n      <td>0.081148</td>\n      <td>0.083424</td>\n      <td>0.083187</td>\n      <td>0.084586</td>\n      <td>0.083220</td>\n    </tr>\n    <tr>\n      <th>check</th>\n      <td>0.084713</td>\n      <td>0.090644</td>\n      <td>0.088666</td>\n      <td>0.086869</td>\n      <td>0.085038</td>\n      <td>0.087435</td>\n      <td>0.091634</td>\n      <td>0.088010</td>\n      <td>0.085868</td>\n      <td>0.089288</td>\n      <td>...</td>\n      <td>0.086496</td>\n      <td>0.091117</td>\n      <td>0.086218</td>\n      <td>0.086828</td>\n      <td>0.086211</td>\n      <td>0.087545</td>\n      <td>0.091965</td>\n      <td>0.083624</td>\n      <td>0.088935</td>\n      <td>0.083730</td>\n    </tr>\n    <tr>\n      <th>day</th>\n      <td>0.063981</td>\n      <td>0.066039</td>\n      <td>0.064580</td>\n      <td>0.067171</td>\n      <td>0.067424</td>\n      <td>0.066501</td>\n      <td>0.073608</td>\n      <td>0.065190</td>\n      <td>0.067399</td>\n      <td>0.068081</td>\n      <td>...</td>\n      <td>0.065800</td>\n      <td>0.069392</td>\n      <td>0.064942</td>\n      <td>0.064707</td>\n      <td>0.066478</td>\n      <td>0.066588</td>\n      <td>0.067914</td>\n      <td>0.066502</td>\n      <td>0.064624</td>\n      <td>0.068875</td>\n    </tr>\n    <tr>\n      <th>did</th>\n      <td>0.069010</td>\n      <td>0.059463</td>\n      <td>0.062672</td>\n      <td>0.060097</td>\n      <td>0.061466</td>\n      <td>0.060028</td>\n      <td>0.057763</td>\n      <td>0.061228</td>\n      <td>0.057161</td>\n      <td>0.061849</td>\n      <td>...</td>\n      <td>0.059801</td>\n      <td>0.058177</td>\n      <td>0.058085</td>\n      <td>0.059492</td>\n      <td>0.060502</td>\n      <td>0.060208</td>\n      <td>0.057867</td>\n      <td>0.058221</td>\n      <td>0.058736</td>\n      <td>0.062416</td>\n    </tr>\n    <tr>\n      <th>enjoy</th>\n      <td>0.056569</td>\n      <td>0.056390</td>\n      <td>0.056095</td>\n      <td>0.057481</td>\n      <td>0.057828</td>\n      <td>0.057457</td>\n      <td>0.065243</td>\n      <td>0.057168</td>\n      <td>0.057072</td>\n      <td>0.056873</td>\n      <td>...</td>\n      <td>0.059388</td>\n      <td>0.061095</td>\n      <td>0.063313</td>\n      <td>0.056340</td>\n      <td>0.057703</td>\n      <td>0.057127</td>\n      <td>0.056057</td>\n      <td>0.056650</td>\n      <td>0.056140</td>\n      <td>0.056999</td>\n    </tr>\n    <tr>\n      <th>good</th>\n      <td>0.241609</td>\n      <td>0.234526</td>\n      <td>0.244435</td>\n      <td>0.240639</td>\n      <td>0.242084</td>\n      <td>0.237324</td>\n      <td>0.231583</td>\n      <td>0.239703</td>\n      <td>0.230516</td>\n      <td>0.232578</td>\n      <td>...</td>\n      <td>0.237050</td>\n      <td>0.228670</td>\n      <td>0.239231</td>\n      <td>0.244478</td>\n      <td>0.236255</td>\n      <td>0.237589</td>\n      <td>0.238119</td>\n      <td>0.236497</td>\n      <td>0.241506</td>\n      <td>0.232954</td>\n    </tr>\n    <tr>\n      <th>great</th>\n      <td>0.137276</td>\n      <td>0.140868</td>\n      <td>0.141685</td>\n      <td>0.143314</td>\n      <td>0.145994</td>\n      <td>0.142050</td>\n      <td>0.153280</td>\n      <td>0.144915</td>\n      <td>0.162648</td>\n      <td>0.146934</td>\n      <td>...</td>\n      <td>0.141895</td>\n      <td>0.149373</td>\n      <td>0.138873</td>\n      <td>0.138090</td>\n      <td>0.143143</td>\n      <td>0.142364</td>\n      <td>0.138991</td>\n      <td>0.146593</td>\n      <td>0.140049</td>\n      <td>0.147788</td>\n    </tr>\n    <tr>\n      <th>haven</th>\n      <td>0.085307</td>\n      <td>0.091002</td>\n      <td>0.093368</td>\n      <td>0.089080</td>\n      <td>0.087781</td>\n      <td>0.087026</td>\n      <td>0.096032</td>\n      <td>0.090301</td>\n      <td>0.092396</td>\n      <td>0.086839</td>\n      <td>...</td>\n      <td>0.086367</td>\n      <td>0.098713</td>\n      <td>0.087703</td>\n      <td>0.088089</td>\n      <td>0.088367</td>\n      <td>0.089353</td>\n      <td>0.087935</td>\n      <td>0.086964</td>\n      <td>0.092091</td>\n      <td>0.084955</td>\n    </tr>\n    <tr>\n      <th>heard</th>\n      <td>0.066156</td>\n      <td>0.064463</td>\n      <td>0.066382</td>\n      <td>0.063570</td>\n      <td>0.064001</td>\n      <td>0.063581</td>\n      <td>0.062324</td>\n      <td>0.067102</td>\n      <td>0.061790</td>\n      <td>0.062975</td>\n      <td>...</td>\n      <td>0.063508</td>\n      <td>0.066844</td>\n      <td>0.062077</td>\n      <td>0.062604</td>\n      <td>0.062917</td>\n      <td>0.063392</td>\n      <td>0.061972</td>\n      <td>0.061388</td>\n      <td>0.067995</td>\n      <td>0.069783</td>\n    </tr>\n    <tr>\n      <th>hello</th>\n      <td>0.071780</td>\n      <td>0.074870</td>\n      <td>0.075675</td>\n      <td>0.073843</td>\n      <td>0.074234</td>\n      <td>0.074771</td>\n      <td>0.076971</td>\n      <td>0.073423</td>\n      <td>0.073849</td>\n      <td>0.073864</td>\n      <td>...</td>\n      <td>0.073567</td>\n      <td>0.074179</td>\n      <td>0.074357</td>\n      <td>0.072527</td>\n      <td>0.073683</td>\n      <td>0.074530</td>\n      <td>0.073120</td>\n      <td>0.072764</td>\n      <td>0.072745</td>\n      <td>0.073983</td>\n    </tr>\n    <tr>\n      <th>help</th>\n      <td>0.075927</td>\n      <td>0.076620</td>\n      <td>0.079915</td>\n      <td>0.077049</td>\n      <td>0.076108</td>\n      <td>0.080807</td>\n      <td>0.080806</td>\n      <td>0.078995</td>\n      <td>0.076426</td>\n      <td>0.076599</td>\n      <td>...</td>\n      <td>0.078180</td>\n      <td>0.075759</td>\n      <td>0.080434</td>\n      <td>0.078104</td>\n      <td>0.076531</td>\n      <td>0.077398</td>\n      <td>0.075335</td>\n      <td>0.075339</td>\n      <td>0.075437</td>\n      <td>0.081395</td>\n    </tr>\n    <tr>\n      <th>hi</th>\n      <td>0.085625</td>\n      <td>0.082834</td>\n      <td>0.085017</td>\n      <td>0.083808</td>\n      <td>0.084028</td>\n      <td>0.083372</td>\n      <td>0.083167</td>\n      <td>0.082933</td>\n      <td>0.083457</td>\n      <td>0.084266</td>\n      <td>...</td>\n      <td>0.084860</td>\n      <td>0.083414</td>\n      <td>0.082425</td>\n      <td>0.082176</td>\n      <td>0.083979</td>\n      <td>0.083896</td>\n      <td>0.082829</td>\n      <td>0.083708</td>\n      <td>0.084524</td>\n      <td>0.086210</td>\n    </tr>\n    <tr>\n      <th>hope</th>\n      <td>0.055973</td>\n      <td>0.056451</td>\n      <td>0.055987</td>\n      <td>0.057507</td>\n      <td>0.057824</td>\n      <td>0.056510</td>\n      <td>0.058158</td>\n      <td>0.057885</td>\n      <td>0.056762</td>\n      <td>0.057011</td>\n      <td>...</td>\n      <td>0.058078</td>\n      <td>0.057017</td>\n      <td>0.057310</td>\n      <td>0.057959</td>\n      <td>0.057534</td>\n      <td>0.056932</td>\n      <td>0.056154</td>\n      <td>0.056294</td>\n      <td>0.055793</td>\n      <td>0.056382</td>\n    </tr>\n    <tr>\n      <th>kind</th>\n      <td>0.066809</td>\n      <td>0.068619</td>\n      <td>0.066388</td>\n      <td>0.068187</td>\n      <td>0.068257</td>\n      <td>0.067744</td>\n      <td>0.066656</td>\n      <td>0.066348</td>\n      <td>0.065837</td>\n      <td>0.068399</td>\n      <td>...</td>\n      <td>0.067739</td>\n      <td>0.066313</td>\n      <td>0.066877</td>\n      <td>0.067620</td>\n      <td>0.068748</td>\n      <td>0.068756</td>\n      <td>0.068062</td>\n      <td>0.070078</td>\n      <td>0.069758</td>\n      <td>0.066611</td>\n    </tr>\n    <tr>\n      <th>know</th>\n      <td>0.060548</td>\n      <td>0.059673</td>\n      <td>0.059669</td>\n      <td>0.061518</td>\n      <td>0.059713</td>\n      <td>0.062499</td>\n      <td>0.057967</td>\n      <td>0.059087</td>\n      <td>0.058178</td>\n      <td>0.059928</td>\n      <td>...</td>\n      <td>0.063120</td>\n      <td>0.063268</td>\n      <td>0.059007</td>\n      <td>0.059899</td>\n      <td>0.061207</td>\n      <td>0.060675</td>\n      <td>0.064017</td>\n      <td>0.059339</td>\n      <td>0.059102</td>\n      <td>0.064140</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0.260856</td>\n      <td>0.261079</td>\n      <td>0.242690</td>\n      <td>0.251123</td>\n      <td>0.245456</td>\n      <td>0.256354</td>\n      <td>0.243933</td>\n      <td>0.247775</td>\n      <td>0.241631</td>\n      <td>0.251013</td>\n      <td>...</td>\n      <td>0.251024</td>\n      <td>0.244608</td>\n      <td>0.252169</td>\n      <td>0.251745</td>\n      <td>0.256100</td>\n      <td>0.255720</td>\n      <td>0.254759</td>\n      <td>0.252351</td>\n      <td>0.252592</td>\n      <td>0.254271</td>\n    </tr>\n    <tr>\n      <th>liked</th>\n      <td>0.088171</td>\n      <td>0.083249</td>\n      <td>0.090089</td>\n      <td>0.085610</td>\n      <td>0.085532</td>\n      <td>0.085921</td>\n      <td>0.087647</td>\n      <td>0.086110</td>\n      <td>0.082870</td>\n      <td>0.082067</td>\n      <td>...</td>\n      <td>0.086015</td>\n      <td>0.087901</td>\n      <td>0.081906</td>\n      <td>0.084862</td>\n      <td>0.085124</td>\n      <td>0.084559</td>\n      <td>0.082956</td>\n      <td>0.080826</td>\n      <td>0.088146</td>\n      <td>0.081311</td>\n    </tr>\n    <tr>\n      <th>ll</th>\n      <td>0.070876</td>\n      <td>0.075324</td>\n      <td>0.071120</td>\n      <td>0.071456</td>\n      <td>0.069562</td>\n      <td>0.071502</td>\n      <td>0.072199</td>\n      <td>0.071057</td>\n      <td>0.069853</td>\n      <td>0.073817</td>\n      <td>...</td>\n      <td>0.071224</td>\n      <td>0.072359</td>\n      <td>0.073549</td>\n      <td>0.070375</td>\n      <td>0.071758</td>\n      <td>0.072389</td>\n      <td>0.076962</td>\n      <td>0.068414</td>\n      <td>0.071731</td>\n      <td>0.069506</td>\n    </tr>\n    <tr>\n      <th>looking</th>\n      <td>0.093841</td>\n      <td>0.096813</td>\n      <td>0.096148</td>\n      <td>0.097325</td>\n      <td>0.097315</td>\n      <td>0.096554</td>\n      <td>0.101929</td>\n      <td>0.094250</td>\n      <td>0.096110</td>\n      <td>0.097738</td>\n      <td>...</td>\n      <td>0.097510</td>\n      <td>0.102726</td>\n      <td>0.093726</td>\n      <td>0.095825</td>\n      <td>0.095370</td>\n      <td>0.096610</td>\n      <td>0.097673</td>\n      <td>0.093820</td>\n      <td>0.096947</td>\n      <td>0.096750</td>\n    </tr>\n    <tr>\n      <th>love</th>\n      <td>0.093400</td>\n      <td>0.094157</td>\n      <td>0.093276</td>\n      <td>0.100696</td>\n      <td>0.099355</td>\n      <td>0.097598</td>\n      <td>0.093881</td>\n      <td>0.093866</td>\n      <td>0.095449</td>\n      <td>0.109037</td>\n      <td>...</td>\n      <td>0.095294</td>\n      <td>0.093763</td>\n      <td>0.108415</td>\n      <td>0.104397</td>\n      <td>0.097083</td>\n      <td>0.097327</td>\n      <td>0.094830</td>\n      <td>0.117418</td>\n      <td>0.097292</td>\n      <td>0.094678</td>\n    </tr>\n    <tr>\n      <th>loved</th>\n      <td>0.059627</td>\n      <td>0.059962</td>\n      <td>0.060581</td>\n      <td>0.062544</td>\n      <td>0.066547</td>\n      <td>0.060754</td>\n      <td>0.061845</td>\n      <td>0.060751</td>\n      <td>0.062194</td>\n      <td>0.062684</td>\n      <td>...</td>\n      <td>0.061003</td>\n      <td>0.066279</td>\n      <td>0.060803</td>\n      <td>0.060861</td>\n      <td>0.061280</td>\n      <td>0.061712</td>\n      <td>0.059867</td>\n      <td>0.064877</td>\n      <td>0.060250</td>\n      <td>0.060934</td>\n    </tr>\n    <tr>\n      <th>movie</th>\n      <td>0.170533</td>\n      <td>0.168060</td>\n      <td>0.171154</td>\n      <td>0.168641</td>\n      <td>0.172231</td>\n      <td>0.168947</td>\n      <td>0.161839</td>\n      <td>0.161414</td>\n      <td>0.178253</td>\n      <td>0.166166</td>\n      <td>...</td>\n      <td>0.170020</td>\n      <td>0.161262</td>\n      <td>0.161461</td>\n      <td>0.169255</td>\n      <td>0.167216</td>\n      <td>0.167520</td>\n      <td>0.165830</td>\n      <td>0.165193</td>\n      <td>0.164022</td>\n      <td>0.175361</td>\n    </tr>\n    <tr>\n      <th>movies</th>\n      <td>0.157122</td>\n      <td>0.160080</td>\n      <td>0.159727</td>\n      <td>0.160169</td>\n      <td>0.160762</td>\n      <td>0.161148</td>\n      <td>0.158631</td>\n      <td>0.161542</td>\n      <td>0.158630</td>\n      <td>0.164020</td>\n      <td>...</td>\n      <td>0.161353</td>\n      <td>0.155150</td>\n      <td>0.162184</td>\n      <td>0.167592</td>\n      <td>0.163768</td>\n      <td>0.160730</td>\n      <td>0.167594</td>\n      <td>0.171501</td>\n      <td>0.165449</td>\n      <td>0.157358</td>\n    </tr>\n    <tr>\n      <th>really</th>\n      <td>0.110483</td>\n      <td>0.109287</td>\n      <td>0.107712</td>\n      <td>0.109011</td>\n      <td>0.109164</td>\n      <td>0.111485</td>\n      <td>0.119317</td>\n      <td>0.111048</td>\n      <td>0.107216</td>\n      <td>0.106378</td>\n      <td>...</td>\n      <td>0.108240</td>\n      <td>0.110971</td>\n      <td>0.111381</td>\n      <td>0.106636</td>\n      <td>0.109329</td>\n      <td>0.108761</td>\n      <td>0.110760</td>\n      <td>0.105557</td>\n      <td>0.109542</td>\n      <td>0.104800</td>\n    </tr>\n    <tr>\n      <th>recommend</th>\n      <td>0.063207</td>\n      <td>0.064479</td>\n      <td>0.064442</td>\n      <td>0.065154</td>\n      <td>0.064895</td>\n      <td>0.064225</td>\n      <td>0.062759</td>\n      <td>0.066904</td>\n      <td>0.074264</td>\n      <td>0.063793</td>\n      <td>...</td>\n      <td>0.067208</td>\n      <td>0.062101</td>\n      <td>0.066804</td>\n      <td>0.064262</td>\n      <td>0.065716</td>\n      <td>0.065085</td>\n      <td>0.070511</td>\n      <td>0.064351</td>\n      <td>0.067786</td>\n      <td>0.064807</td>\n    </tr>\n    <tr>\n      <th>saw</th>\n      <td>0.061073</td>\n      <td>0.054875</td>\n      <td>0.063082</td>\n      <td>0.055555</td>\n      <td>0.057539</td>\n      <td>0.054694</td>\n      <td>0.054083</td>\n      <td>0.057907</td>\n      <td>0.053621</td>\n      <td>0.055955</td>\n      <td>...</td>\n      <td>0.056193</td>\n      <td>0.054427</td>\n      <td>0.053799</td>\n      <td>0.060170</td>\n      <td>0.056693</td>\n      <td>0.055600</td>\n      <td>0.054224</td>\n      <td>0.053604</td>\n      <td>0.055222</td>\n      <td>0.054888</td>\n    </tr>\n    <tr>\n      <th>seen</th>\n      <td>0.200570</td>\n      <td>0.209690</td>\n      <td>0.205850</td>\n      <td>0.206133</td>\n      <td>0.206540</td>\n      <td>0.204107</td>\n      <td>0.209607</td>\n      <td>0.208152</td>\n      <td>0.215208</td>\n      <td>0.203961</td>\n      <td>...</td>\n      <td>0.204343</td>\n      <td>0.216950</td>\n      <td>0.208121</td>\n      <td>0.204338</td>\n      <td>0.206446</td>\n      <td>0.207858</td>\n      <td>0.206187</td>\n      <td>0.203577</td>\n      <td>0.205796</td>\n      <td>0.197589</td>\n    </tr>\n    <tr>\n      <th>suggestions</th>\n      <td>0.069231</td>\n      <td>0.065972</td>\n      <td>0.069209</td>\n      <td>0.067995</td>\n      <td>0.066600</td>\n      <td>0.066469</td>\n      <td>0.070206</td>\n      <td>0.069213</td>\n      <td>0.069560</td>\n      <td>0.067299</td>\n      <td>...</td>\n      <td>0.066982</td>\n      <td>0.067371</td>\n      <td>0.069593</td>\n      <td>0.067817</td>\n      <td>0.067702</td>\n      <td>0.067316</td>\n      <td>0.065754</td>\n      <td>0.068595</td>\n      <td>0.066765</td>\n      <td>0.066070</td>\n    </tr>\n    <tr>\n      <th>thank</th>\n      <td>0.068285</td>\n      <td>0.073163</td>\n      <td>0.074600</td>\n      <td>0.071646</td>\n      <td>0.072001</td>\n      <td>0.071438</td>\n      <td>0.070871</td>\n      <td>0.074389</td>\n      <td>0.071307</td>\n      <td>0.071583</td>\n      <td>...</td>\n      <td>0.074676</td>\n      <td>0.071430</td>\n      <td>0.071473</td>\n      <td>0.070750</td>\n      <td>0.071175</td>\n      <td>0.071182</td>\n      <td>0.069530</td>\n      <td>0.073208</td>\n      <td>0.070461</td>\n      <td>0.077006</td>\n    </tr>\n    <tr>\n      <th>thanks</th>\n      <td>0.092376</td>\n      <td>0.084424</td>\n      <td>0.084489</td>\n      <td>0.086051</td>\n      <td>0.085936</td>\n      <td>0.086323</td>\n      <td>0.088481</td>\n      <td>0.087447</td>\n      <td>0.087876</td>\n      <td>0.087708</td>\n      <td>...</td>\n      <td>0.084559</td>\n      <td>0.085171</td>\n      <td>0.089806</td>\n      <td>0.085006</td>\n      <td>0.086347</td>\n      <td>0.086451</td>\n      <td>0.088838</td>\n      <td>0.084340</td>\n      <td>0.086949</td>\n      <td>0.087465</td>\n    </tr>\n    <tr>\n      <th>think</th>\n      <td>0.099975</td>\n      <td>0.093525</td>\n      <td>0.091946</td>\n      <td>0.093073</td>\n      <td>0.093730</td>\n      <td>0.094201</td>\n      <td>0.091521</td>\n      <td>0.091468</td>\n      <td>0.091086</td>\n      <td>0.094247</td>\n      <td>...</td>\n      <td>0.096407</td>\n      <td>0.091432</td>\n      <td>0.091783</td>\n      <td>0.091643</td>\n      <td>0.093908</td>\n      <td>0.094304</td>\n      <td>0.091256</td>\n      <td>0.090837</td>\n      <td>0.093875</td>\n      <td>0.091520</td>\n    </tr>\n    <tr>\n      <th>ve</th>\n      <td>0.073811</td>\n      <td>0.076842</td>\n      <td>0.073832</td>\n      <td>0.074381</td>\n      <td>0.074623</td>\n      <td>0.074070</td>\n      <td>0.074514</td>\n      <td>0.077606</td>\n      <td>0.075160</td>\n      <td>0.075289</td>\n      <td>...</td>\n      <td>0.077670</td>\n      <td>0.076730</td>\n      <td>0.077335</td>\n      <td>0.075389</td>\n      <td>0.075228</td>\n      <td>0.076322</td>\n      <td>0.074535</td>\n      <td>0.072069</td>\n      <td>0.074300</td>\n      <td>0.073278</td>\n    </tr>\n    <tr>\n      <th>watch</th>\n      <td>0.091920</td>\n      <td>0.094397</td>\n      <td>0.099178</td>\n      <td>0.094663</td>\n      <td>0.093892</td>\n      <td>0.096185</td>\n      <td>0.095627</td>\n      <td>0.096417</td>\n      <td>0.093303</td>\n      <td>0.092824</td>\n      <td>...</td>\n      <td>0.095504</td>\n      <td>0.099886</td>\n      <td>0.092034</td>\n      <td>0.094861</td>\n      <td>0.095182</td>\n      <td>0.094059</td>\n      <td>0.091990</td>\n      <td>0.096035</td>\n      <td>0.091927</td>\n      <td>0.108775</td>\n    </tr>\n    <tr>\n      <th>yes</th>\n      <td>0.081464</td>\n      <td>0.081432</td>\n      <td>0.078356</td>\n      <td>0.079103</td>\n      <td>0.080106</td>\n      <td>0.077656</td>\n      <td>0.076103</td>\n      <td>0.079980</td>\n      <td>0.081831</td>\n      <td>0.079966</td>\n      <td>...</td>\n      <td>0.078912</td>\n      <td>0.081202</td>\n      <td>0.077537</td>\n      <td>0.082192</td>\n      <td>0.080084</td>\n      <td>0.079098</td>\n      <td>0.076338</td>\n      <td>0.080115</td>\n      <td>0.076961</td>\n      <td>0.080652</td>\n    </tr>\n  </tbody>\n</table>\n<p>34 rows × 6311 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_rating(grouped_arr, word_sim_arr):\n",
    "    ratings_pred = grouped_arr.dot(word_sim_arr) / np.array([np.abs(word_sim_arr).sum(axis=1)])\n",
    "    return ratings_pred\n",
    "\n",
    "word_pred = predict_rating(df_grouped.transpose().values, df_word_sim.values)\n",
    "df_word_pred = pd.DataFrame(word_pred, index = df_grouped.transpose().index, columns=df_grouped.transpose().columns)\n",
    "df_word_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "구한 코사인 유사도와 df_grouped데이터를 내적하여서, 단어와 영화 사이 예측 값을 구하였습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "              bye     check       day       did     enjoy      good     great  \\\nid                                                                              \n0104807  0.083689  0.084713  0.063981  0.069010  0.056569  0.241609  0.137276   \n1000     0.079910  0.090644  0.066039  0.059463  0.056390  0.234526  0.140868   \n100026   0.082787  0.088666  0.064580  0.062672  0.056095  0.244435  0.141685   \n100030   0.080793  0.086869  0.067171  0.060097  0.057481  0.240639  0.143314   \n100043   0.082266  0.085038  0.067424  0.061466  0.057828  0.242084  0.145994   \n...           ...       ...       ...       ...       ...       ...       ...   \n99910    0.081148  0.087545  0.066588  0.060208  0.057127  0.237589  0.142364   \n99955    0.083424  0.091965  0.067914  0.057867  0.056057  0.238119  0.138991   \n99966    0.083187  0.083624  0.066502  0.058221  0.056650  0.236497  0.146593   \n99975    0.084586  0.088935  0.064624  0.058736  0.056140  0.241506  0.140049   \n99998    0.083220  0.083730  0.068875  0.062416  0.056999  0.232954  0.147788   \n\n            haven     heard     hello  ...  recommend       saw      seen  \\\nid                                     ...                                  \n0104807  0.085307  0.066156  0.071780  ...   0.063207  0.061073  0.200570   \n1000     0.091002  0.064463  0.074870  ...   0.064479  0.054875  0.209690   \n100026   0.093368  0.066382  0.075675  ...   0.064442  0.063082  0.205850   \n100030   0.089080  0.063570  0.073843  ...   0.065154  0.055555  0.206133   \n100043   0.087781  0.064001  0.074234  ...   0.064895  0.057539  0.206540   \n...           ...       ...       ...  ...        ...       ...       ...   \n99910    0.089353  0.063392  0.074530  ...   0.065085  0.055600  0.207858   \n99955    0.087935  0.061972  0.073120  ...   0.070511  0.054224  0.206187   \n99966    0.086964  0.061388  0.072764  ...   0.064351  0.053604  0.203577   \n99975    0.092091  0.067995  0.072745  ...   0.067786  0.055222  0.205796   \n99998    0.084955  0.069783  0.073983  ...   0.064807  0.054888  0.197589   \n\n         suggestions     thank    thanks     think        ve     watch  \\\nid                                                                       \n0104807     0.069231  0.068285  0.092376  0.099975  0.073811  0.091920   \n1000        0.065972  0.073163  0.084424  0.093525  0.076842  0.094397   \n100026      0.069209  0.074600  0.084489  0.091946  0.073832  0.099178   \n100030      0.067995  0.071646  0.086051  0.093073  0.074381  0.094663   \n100043      0.066600  0.072001  0.085936  0.093730  0.074623  0.093892   \n...              ...       ...       ...       ...       ...       ...   \n99910       0.067316  0.071182  0.086451  0.094304  0.076322  0.094059   \n99955       0.065754  0.069530  0.088838  0.091256  0.074535  0.091990   \n99966       0.068595  0.073208  0.084340  0.090837  0.072069  0.096035   \n99975       0.066765  0.070461  0.086949  0.093875  0.074300  0.091927   \n99998       0.066070  0.077006  0.087465  0.091520  0.073278  0.108775   \n\n              yes  \nid                 \n0104807  0.081464  \n1000     0.081432  \n100026   0.078356  \n100030   0.079103  \n100043   0.080106  \n...           ...  \n99910    0.079098  \n99955    0.076338  \n99966    0.080115  \n99975    0.076961  \n99998    0.080652  \n\n[6311 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bye</th>\n      <th>check</th>\n      <th>day</th>\n      <th>did</th>\n      <th>enjoy</th>\n      <th>good</th>\n      <th>great</th>\n      <th>haven</th>\n      <th>heard</th>\n      <th>hello</th>\n      <th>...</th>\n      <th>recommend</th>\n      <th>saw</th>\n      <th>seen</th>\n      <th>suggestions</th>\n      <th>thank</th>\n      <th>thanks</th>\n      <th>think</th>\n      <th>ve</th>\n      <th>watch</th>\n      <th>yes</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0104807</th>\n      <td>0.083689</td>\n      <td>0.084713</td>\n      <td>0.063981</td>\n      <td>0.069010</td>\n      <td>0.056569</td>\n      <td>0.241609</td>\n      <td>0.137276</td>\n      <td>0.085307</td>\n      <td>0.066156</td>\n      <td>0.071780</td>\n      <td>...</td>\n      <td>0.063207</td>\n      <td>0.061073</td>\n      <td>0.200570</td>\n      <td>0.069231</td>\n      <td>0.068285</td>\n      <td>0.092376</td>\n      <td>0.099975</td>\n      <td>0.073811</td>\n      <td>0.091920</td>\n      <td>0.081464</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>0.079910</td>\n      <td>0.090644</td>\n      <td>0.066039</td>\n      <td>0.059463</td>\n      <td>0.056390</td>\n      <td>0.234526</td>\n      <td>0.140868</td>\n      <td>0.091002</td>\n      <td>0.064463</td>\n      <td>0.074870</td>\n      <td>...</td>\n      <td>0.064479</td>\n      <td>0.054875</td>\n      <td>0.209690</td>\n      <td>0.065972</td>\n      <td>0.073163</td>\n      <td>0.084424</td>\n      <td>0.093525</td>\n      <td>0.076842</td>\n      <td>0.094397</td>\n      <td>0.081432</td>\n    </tr>\n    <tr>\n      <th>100026</th>\n      <td>0.082787</td>\n      <td>0.088666</td>\n      <td>0.064580</td>\n      <td>0.062672</td>\n      <td>0.056095</td>\n      <td>0.244435</td>\n      <td>0.141685</td>\n      <td>0.093368</td>\n      <td>0.066382</td>\n      <td>0.075675</td>\n      <td>...</td>\n      <td>0.064442</td>\n      <td>0.063082</td>\n      <td>0.205850</td>\n      <td>0.069209</td>\n      <td>0.074600</td>\n      <td>0.084489</td>\n      <td>0.091946</td>\n      <td>0.073832</td>\n      <td>0.099178</td>\n      <td>0.078356</td>\n    </tr>\n    <tr>\n      <th>100030</th>\n      <td>0.080793</td>\n      <td>0.086869</td>\n      <td>0.067171</td>\n      <td>0.060097</td>\n      <td>0.057481</td>\n      <td>0.240639</td>\n      <td>0.143314</td>\n      <td>0.089080</td>\n      <td>0.063570</td>\n      <td>0.073843</td>\n      <td>...</td>\n      <td>0.065154</td>\n      <td>0.055555</td>\n      <td>0.206133</td>\n      <td>0.067995</td>\n      <td>0.071646</td>\n      <td>0.086051</td>\n      <td>0.093073</td>\n      <td>0.074381</td>\n      <td>0.094663</td>\n      <td>0.079103</td>\n    </tr>\n    <tr>\n      <th>100043</th>\n      <td>0.082266</td>\n      <td>0.085038</td>\n      <td>0.067424</td>\n      <td>0.061466</td>\n      <td>0.057828</td>\n      <td>0.242084</td>\n      <td>0.145994</td>\n      <td>0.087781</td>\n      <td>0.064001</td>\n      <td>0.074234</td>\n      <td>...</td>\n      <td>0.064895</td>\n      <td>0.057539</td>\n      <td>0.206540</td>\n      <td>0.066600</td>\n      <td>0.072001</td>\n      <td>0.085936</td>\n      <td>0.093730</td>\n      <td>0.074623</td>\n      <td>0.093892</td>\n      <td>0.080106</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99910</th>\n      <td>0.081148</td>\n      <td>0.087545</td>\n      <td>0.066588</td>\n      <td>0.060208</td>\n      <td>0.057127</td>\n      <td>0.237589</td>\n      <td>0.142364</td>\n      <td>0.089353</td>\n      <td>0.063392</td>\n      <td>0.074530</td>\n      <td>...</td>\n      <td>0.065085</td>\n      <td>0.055600</td>\n      <td>0.207858</td>\n      <td>0.067316</td>\n      <td>0.071182</td>\n      <td>0.086451</td>\n      <td>0.094304</td>\n      <td>0.076322</td>\n      <td>0.094059</td>\n      <td>0.079098</td>\n    </tr>\n    <tr>\n      <th>99955</th>\n      <td>0.083424</td>\n      <td>0.091965</td>\n      <td>0.067914</td>\n      <td>0.057867</td>\n      <td>0.056057</td>\n      <td>0.238119</td>\n      <td>0.138991</td>\n      <td>0.087935</td>\n      <td>0.061972</td>\n      <td>0.073120</td>\n      <td>...</td>\n      <td>0.070511</td>\n      <td>0.054224</td>\n      <td>0.206187</td>\n      <td>0.065754</td>\n      <td>0.069530</td>\n      <td>0.088838</td>\n      <td>0.091256</td>\n      <td>0.074535</td>\n      <td>0.091990</td>\n      <td>0.076338</td>\n    </tr>\n    <tr>\n      <th>99966</th>\n      <td>0.083187</td>\n      <td>0.083624</td>\n      <td>0.066502</td>\n      <td>0.058221</td>\n      <td>0.056650</td>\n      <td>0.236497</td>\n      <td>0.146593</td>\n      <td>0.086964</td>\n      <td>0.061388</td>\n      <td>0.072764</td>\n      <td>...</td>\n      <td>0.064351</td>\n      <td>0.053604</td>\n      <td>0.203577</td>\n      <td>0.068595</td>\n      <td>0.073208</td>\n      <td>0.084340</td>\n      <td>0.090837</td>\n      <td>0.072069</td>\n      <td>0.096035</td>\n      <td>0.080115</td>\n    </tr>\n    <tr>\n      <th>99975</th>\n      <td>0.084586</td>\n      <td>0.088935</td>\n      <td>0.064624</td>\n      <td>0.058736</td>\n      <td>0.056140</td>\n      <td>0.241506</td>\n      <td>0.140049</td>\n      <td>0.092091</td>\n      <td>0.067995</td>\n      <td>0.072745</td>\n      <td>...</td>\n      <td>0.067786</td>\n      <td>0.055222</td>\n      <td>0.205796</td>\n      <td>0.066765</td>\n      <td>0.070461</td>\n      <td>0.086949</td>\n      <td>0.093875</td>\n      <td>0.074300</td>\n      <td>0.091927</td>\n      <td>0.076961</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>0.083220</td>\n      <td>0.083730</td>\n      <td>0.068875</td>\n      <td>0.062416</td>\n      <td>0.056999</td>\n      <td>0.232954</td>\n      <td>0.147788</td>\n      <td>0.084955</td>\n      <td>0.069783</td>\n      <td>0.073983</td>\n      <td>...</td>\n      <td>0.064807</td>\n      <td>0.054888</td>\n      <td>0.197589</td>\n      <td>0.066070</td>\n      <td>0.077006</td>\n      <td>0.087465</td>\n      <td>0.091520</td>\n      <td>0.073278</td>\n      <td>0.108775</td>\n      <td>0.080652</td>\n    </tr>\n  </tbody>\n</table>\n<p>6311 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위형식을 가지고, 데이터를 살짝 변경해서 만들어보면 될듯?\n",
    "df_word_pred.transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "transpose를 취하면 다음과 같은 데이터프레임을 얻을수 있습니다.\n",
    "\n",
    "그 다음, 영화를 입력 받으면, 위 데이터프레임과의 유사도를 확인한 다음 상위 5개를 출력하는 방식으로 구현하였는데, 이렇게 하는게 맞을까요???\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Simliarity\n",
    "Compute the similarity between words for recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation function\n",
    "* param:\n",
    "    * data: array, vector space of texts.\n",
    "    * mv: target movie's index\n",
    "    * length: maximum length of recommendation\n",
    "        * default: 5\n",
    "    * simf: consine similarity function\n",
    "        * default: dot(X, y) / (normalize(X) * normalize(Y) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VVlR2925P7RH"
   },
   "outputs": [],
   "source": [
    "# Note: the consine similarity function's denominator has 1e-7 minimum value to avoid the divbyzero.\n",
    "def recommend(data, mv, length=5, simf=lambda X, Y: np.dot(X,Y)/((norm(X)*norm(Y)) + 1e-7)):\n",
    "    sim = []\n",
    "\n",
    "    if df.loc[df['movieid'] == mv].empty:\n",
    "        return sim\n",
    "    \n",
    "    idx = df[df['movieid'] == mv].index.values[0]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if idx != i:\n",
    "            sim.append((simf(data[i], data[idx]), df.loc[i]['movieid']))\n",
    "    \n",
    "    sim.sort()\n",
    "    sim.reverse()\n",
    "    return sim[:length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "Try a recommendation with movie id 80067 (= Toy Story (1995))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlYausP8sqc1",
    "outputId": "83ef1e13-1589-48cf-8be6-4f56b4238a13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Similarity Movie Index\n0    1.000000       81792\n1    1.000000      182731\n2    1.000000      154844\n3    1.000000      133249\n4    0.773555       81385\n5    0.773555        2000\n6    0.773555      199831\n7    0.773555      199831\n8    0.773555      189328\n9    0.773555      177112",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Similarity</th>\n      <th>Movie Index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>81792</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.000000</td>\n      <td>182731</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.000000</td>\n      <td>154844</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.000000</td>\n      <td>133249</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.773555</td>\n      <td>81385</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.773555</td>\n      <td>2000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.773555</td>\n      <td>199831</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.773555</td>\n      <td>199831</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.773555</td>\n      <td>189328</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.773555</td>\n      <td>177112</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(recommend(tfidf_mat, '80067', 10), columns=['Similarity', 'Movie Index'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
